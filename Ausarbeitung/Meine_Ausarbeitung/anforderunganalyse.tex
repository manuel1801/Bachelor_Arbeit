\chapter{Anforderungen und Analyse}\label{kap:anforderunganalyse}

\section{Ziel der Arbeit}\label{sec:zielderarbeit}


Wie in der Einleitung \ref{kap:einleitung} beschrieben, soll 
ein CNN Basiertes System zur Wildtiererkennung entwickelt 
werden, das für dir Inferenz den Neural Compute Stick 2 verwendet.
Dabei sollte neben der reinen Erkennung auch eine Lokalisierung 
der erkannten Tiere im Bild stattfinden. Gängige Techniken dafür 
werden im nächsten Abschnitt erläutert.
\\
Dabei soll das Deep Learing Modell im Rahmen der gegebenen 
möglichkeiten und Limitierungen der Hardware möglichst 
genau und Robust sein, sodass es auch für die graustufen 
Bilder der Infrarot Kamera zuverlässig funktioniert.
Da eine erhöht Genauigkeit auch immer mit einer größeren
Latenz für der Inferenzzeit einhergeht war dies ein mit 
zu berücksichtigender Punkt.
\\
Neben training und evaluierung eines geeigneten Deep Learning Modells,
war die Implementierung der Anwendung, welche die Inferenz 
des Modells ausführt ein weiterer Bestandteil der Arbeit.
\\
Diese soll voll autonom auf dem Raspberry Pi laufen,
über eine mobile Netzwerk Verbindung verfügen und 
mittels eines geeigneten Kommunikations Protokolls die 
die erkannten und abgespeicherten Bilder an einen
Heim Pc senden.
Des weiteren sollte eine geeignete Kamera verwendet werden, die 
sowohl normale, als auch Infrarot Aufnahmen machen kann.


\section{Related Work}\label{sec:related_work}

\subsection{Convolutional Neural Networks}

\subsubsection{GoogleLeNet (Inception)}

Eine Möglichkeit die Genauigkeit eines Deep Learning Modell 
zu verbessern, ist es mehr Schichte und/oder eine 
größere Anzahl an Neuronen pro Schicht zu verwenden.

Der Nachteil dabei ist der größere Rechenaufwand, sowie 
die höhere Chance des Overfittings.

Das in \cite{szegedyGoingDeeperConvolutions2014} vorgestellte GoogleLeNet hat mit den Inception 
Modulen einen neuen, effizienteren Asatz gefunden die 
Komplexität und damit die Genauigkeit zu erhöhen.

Die Module bestehen aus parallel ausgeführten 
Convolutional Layern unterschiedlicer Kernel 
Größe ($1\times1, 3\times3, 5\times5$)

Diesen werden zu Dimensionsreduktion häufig 
$1\time1$ Kernel vorgeschaltet.

Um die effizienz weiter zu Steigern wurden in 
der zweiten variante des GoogleLeNet
\cite{szegedyRethinkingInceptionArchitecture2015}
 neben anderen verbesserungen, die 
$ 5\times5$ filter durch 2  $3\times3$ Filter 
ersetzt.

Am ende eines Ineption Moduls werden diese 
in einem \textit{DepthConcat} Layer wieder zusammengeführt.

Dadurch können Features auf unterschiedlicher skalen 
besser zusammengesetuzt und erkannt ewreden.



\begin{minipage}{0.5\textwidth}
    \centering
    \input{Bilder/inception_module.tex}
    \captionof{figure}{Inception Module V1}
    \label{fig:incept_modul}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \input{Bilder/inception_module_v2.tex}
    \captionof{figure}{Inception ModuleV2}
    \label{fig:incept_modul}
\end{minipage}




\subsubsection{Mobilenet}

\cite{howardMobileNetsEfficientConvolutional2017a}

Geringere Komplexität für Mobile oder embedded geräte, indem 
die normale Faltungsoperation mit der \textit{Depthwise Seperable 
Convolutions} ersetzt wird. 
Diese besteht aus einer \textit{Depthwise  
Convolutions}, welche die Faltunge auf 
die drei Farbchannel seperat anwendet udn einer 1times1
\textit{pointwise convolution}, welche die input channel combiniert. 

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{DepthwiseSeperable.png}
%     \caption{DepthwiseSeperable}
%     \label{fig:mobilenet}
% \end{figure}
% %https://www.researchgate.net/publication/326727106/figure/fig1/AS:654646985113600@1533091412204/The-architecture-a-Standard-convolution-filters-b-Depthwise-convolutional-filters-c-1.png




In der zweiten version \cite{sandlerMobileNetV2InvertedResiduals2019} enthalten die 
Hauptbestandteile eine 1times1 conv mit relu, die 
3time3 depthwise gefolgt von einer weitere 1time1 ohne aktivierungsfunktion.

eine \textit{residual connection} unterstütz den gradientenfluss.

%https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{mobilenet_v2.png}
    \caption{MobilenetV2}
    \label{fig:mobilenetv2}
\end{figure}




\subsection{Object Detection}
% papers
% SSD: \cite{liuSSDSingleShot2016}
% speed/acc: \cite{huangSpeedAccuracyTradeOffs2017}
% blogpost: \cite{wengObjectDetectionPart2018}

% Bilder:
% SSD: \cite{SSDSingleShot}
% speed acc: \cite{huangSpeedAccuracyTradeOffs2017}

Wie in \ref{subsec:objdet_det} erwähnt, werden für die 
Object Detection neben einem CNN zur Feature extaction 
weiteere strukturen/framework benötigt zur lokalisierung 
des Objekts im Bild

Die entwicklung immer genauerer und effizienterer 
Frameworks ist Gegenstand aktueller Forschung und lassen 
sich wie in dem übersichtspaper \cite{ouaknineReviewDeepLearning}
beschrieben in die zwei grundarten \textit{Region proposal based} 
und den \textit{Regression/Classification Based} einteilen.



\subsubsection{Faster R-CNN}
Region Proposial Based Ansatz, \dots \cite{renFasterRCNNRealTime2016a}
bei dem das RPN ein Bild als unput erhällt und
daraus mögliche regionen für objekte im Bild findet.

hier in einem Fully Convolutional realisiert, über dessen 
Feature Maps im Sliding winow verfahren vordefinierte Anker
 Boxen konvoliert werden. Der Resultieruende Vektor wird 
 dann in einen binären FC Layer (cls) zur klassifizierung und 
einen box-regressor layer (reg) für die Koordinaten gegeben.


\begin{figure}[H]
    \centering
    \label{fig:faster_rcnn}
    \includegraphics[width=0.8\textwidth]{faster-RCNN_architecture.png}
    \caption{Faster R-CNN, quelle: \cite{ObjectDetectionDummies2017b}}
\end{figure}



\subsubsection{SSD: Single Shot MultiBox Detector}
Ist ein Regression/Classification Based Ansatz ... \cite{liuSSDSingleShot2016}

Dabei werden dem Backbone CNN wetere feature Layer verschiedener größe 
angehänget, welche zusammen mit default anker boxen und einem score 
für die anwesenheit des objakts in der box in einen non max supression 
layer gegeben, welcher die finalen predictions bestimmt.


\begin{figure}[H]
    \centering
    \label{fig:faster_rcnn}
    \includegraphics[width=0.8\textwidth]{ssd_architecture.png}
    \caption{SSD, quelle: \cite{SSDSingleShot}}
\end{figure}

