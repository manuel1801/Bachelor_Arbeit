\chapter{Grundlagen}\label{kap:grundlagen}

%####################  CHAPTER 1: Grundlagen  #################

Dieses Kapitel beschreibt in den ersten Abschnitten die Grundlagen 
des Machine Learnings, und die Funktionsweise Künstlicher Neuronaler Netze,
insbesondere für Computer Vision Anwendungen.
Der letzte Abschnitt behandelt die für die Arbeit verwendete KI taugliche 
Hardware, den Neural Compute Stick 2.



%------------------- SECTION: Machine Learning ------------------

\section{Machine Learning}\label{sec:ml}

Beim Machine Lerining, welches ein Teilgebiet der Computerwissenschafen
ist, geht es um die Erstellung von Algorithmen, die Zusammenhänge in großen
Datenmengen erkennen, ohne explizit darauf programmiert worden zu sein.
Eine Form davon ist das \textit{Supervised Learning}, bei dem das Programm 
neben den Input Daten auch die Zugehörigen Ausgaben erhält um daraus 
Regeln für Zusammenhänge zwischen Ein- und Ausgabe Daten abzuleiten.

Dadurch unterscheidet sich das Vorgehen wesentlich zur klassischen Programmierung,
bei bei der die Regeln vorab definiert werden müssen.

\vspace{0.4cm}
\begin{center}
    \input{Bilder/ml_classic_system.tex}    
\end{center}
\vspace{0.4cm}



Das ableiten der Regeln erfolgt beim Machine Learning in einem 
iterativen Prozess, welcher als Training bezeichnet wird.
Dabei werden die Zusammenhänge zwischen In- und Output Daten 
als mathematische Funktion betrachtet, welche numerisch 
angenähert wird.

Bei einem linearen Zusammenhang, handelt es sich
um eine Regression und bei einem kategorischen um 
eine Klassifizierung.

Weitere Formen neben dem \textit{Supervised Learning} sind das 
\textit{Unsupervised Learning}, bei der das Programm keine Labels 
erhällt, sondern diese selber finden 
soll, oder das \textit{Reinfocement Learning}, bei dem das Programm 
durch interaktion mit der Umwelt bestimmte aufgaben lernt.
Diese techniken wurden in der Bachelor Arbeit nicht verwendet 
und werden daher nicht näher erläutert.

%------------------- SECTION: Neuronale Netze -------------------

\subsection{Künstliche Neuronale Netze} \label{subsec:nn}

Für komplexe Input Daten, wie beispielsweise Bilder, bei denen 
die einzelnen Pixelwerte als Inputs und der Inhalt des Bildes als 
Output dienen, werden in der Regel künstliche Neuronale Netze verwendet.
Diese sind eine Form des Machine Learings und bestehen aus einer 
vielzahl an miteinander verbundener Neuronen. Durch unterschiedlich 
starke Gewichtungen der einzelnen Verbindungen, auch Gewichte genannt, 
können für unterschiedliche Input Daten die entsprechenden Outputs 
gefunden werden.

\begin{figure}[H]
    \centering
    \label{fig:nn}
    \def\svgwidth{0.65\columnwidth}
    \footnotesize
    \input{Bilder/nn.pdf_tex}
\end{figure}

Die richtige gewichtung der Parameter erfolgt dabei in einem iterativen Trainingsprozess, 
welcher aus den drei schritten:


\begin{itemize}
    \item \textit{Forward Pass} anhand aktueller Gewichte vorhersage aus den Inputs treffen
    \item \textit{Fehlerbestimmung} Abweichung zum tatsächlichen werten berechnen
    \item \textit{Backpropagation} minimierung der Fehlerfunktion durch Anpassung der Gewichte
\end{itemize}

besthet und in Abbildung \ref{fig:train} schematisch dargestellt ist.



\begin{figure}[H]
    \centering
    \label{fig:train}
    \input{Bilder/train_workflow.tex}
    \caption{Trainingsablauf NN}
\end{figure}




Durch merhfaches durchlaufen der Schritte wird die Fehlerfunktion soweit minimiert, 
sodass das Modell auch für neue Input Daten die richtigen Aussagen treffen kann.
Die mathematischen berechnungen der einzelnen Schritte werden im folgeden erläutert.


\subsubsection{Forward Pass}
Im \textit{Forward Pass} werden die Inputs durch alle Schichten hindurch 
gereicht, um in der letzten Schicht den gewünschten Output zu liefern.
Dabei erhält jedes Neuron die mit $w_{i}$ gewichteten Ausgabewerte aller
Neuronen der vorherigen Schicht und summiert diese zusammen mit einem konstanten 
Bias Wert $b$ auf. 
Mithilfe einer Aktivierungsfunktion wird der Wert auf einen bestimmenten Bereich 
skalliert \ref{fig:neuron}


\begin{figure}[H]
    \centering
    \label{fig:neuron}
    \input{Bilder/neuron}
    \caption{Einzelnes Perzeptron}
\end{figure}


Um den Forward Pass für eine gesammte Schicht, bestehend aus 
einer Vielzahl an Neuronen, zu berechnen, werden die Schichten 
als Vektoren und die Gewichte als Matrizen dargestellt.

Die Matrixmultiplikation aus dem Vektor der vorherigen 
Schicht $x$ mit der Gecwichtsmatrix $W$ ergibt die Werte
des Vektors $z$ der aktuellen Schicht,

\begin{equation}
    \label{eq:forward}
    z = W^{T}x+b
\end{equation}

Dieser wird dann elementweise einer nichtlinearen Aktivierungsfunktion
$g(z)$ übergeben wird.

Für mittlere Schichten (Hidden Layer) wird dabei oft die in \ref{plot:relu} dargestellte
\textit{ReLU Funktion} verwendet welche positive Werte beibehällt und negative 
Werte zu 0 setzt.

Das hat den Vorteil, \dots

Um für die Outputs einen Wahrscheinlichkeits Wert zwischen 0 und 1 
zu erhalten, wird in der letzten Schicht für eine binäre Klassifikation 
die Sigmoid Funktion (\ref{plot:sidmoid}) verwendet,
welche S-Förmig zwischen 0 und 1 verläuft.

\vspace{1cm}

\begin{minipage}{0.5\textwidth}
    \centering
    \begin{equation*}
        \label{eq:relu}
        g(z) = max\{0,z\}
    \end{equation*}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \begin{equation*}
        \label{eq:sidmoid}
        g(z) = \frac{1}{1 + e^{-x}}
    \end{equation*}    
\end{minipage}

\vspace{1cm}

\begin{minipage}{0.5\textwidth}
    \centering
    \label{plot:relu}
    \input{Bilder/relu.tex}
    \captionof{figure}{ReLU Funktion} 
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \label{plot:sigmoid}
    \input{Bilder/sigmoid.tex}
    \captionof{figure}{Sigmoid Funktion} 
\end{minipage}

\vspace{1cm}


Für eine Kategorische Ausgabe mit mehr als zwei Werten wird 
die Softmax (\ref{eq:softmax}) Funktion verwendet, welche eine Wahrscheinlichkeits 
Verteilung über alle Ausgebe Neuronen generiert.

\begin{equation}
    \label{eq:softmax}
    g(z) = \frac{e^{z}}{\sum e^{x}}
\end{equation}


\subsubsection{Fehlerbestimmung}
Die Abweichung des geschätzten Werts, welche an den Neuronen der letzen Schicht 
vorliegen, zum tatsächlichen Werten, dem Label, wird mithilfe einer geeigneten
Fehlerfunktion bestimmt. Für eine Lineare Regression wird dabei z.B. 
der absolute oder quadratischen Abstand verwendet.

Für Klassifikationsmodelle wird meistens eine logarithmische Fehlerberechnung, wie 
die \textit{Cross Enropy Funktion} \ref{eq:crossentropy} verwendet.

\begin{equation}
    \label{eq:crossentropy}
    L = \hat{y}log(y) + (1 - \hat{y})log(1 - y)
\end{equation}

Durch den Logarithmus wird der Loss um so größer, je weiter die Schätzung $y$ vom 
tatsächlichen Wert $\hat{y}$ abweicht.
%hier plot


\subsubsection{Backpropagation}

Durch Berechnung des Gradienten der Fehlerfunktion kann ermittelt 
werden in welche Richtung die Gewichte angepasst werden müssen,
sodass diese sich im nächsten Durchgang minimiert.

Dafür wird die die Fehlerfunktion $L$ für jede Schicht partiell nach den 
Gewichten $w$ abgeleitet, was wie in gl. \ref{eq:backprop} dargestellt mithilfe der 
Kettenregel über die Aktivierungsfunktion $z$ geschieht.


\begin{equation}
    \label{eq:backprop}
    \frac{\partial L}{\partial w} = \frac{\partial L}{\partial z}\frac{\partial z}{\partial w}
\end{equation}
Mit dem ermittelten Gradienten werden dann die Gewichte nach Gleichung \ref{eq:update_wieghts} angepasst.
\begin{equation}
    \label{eq:update_wieghts}
    w  \leftarrow w - \eta \frac{\partial L}{\partial w}
\end{equation}

wobei die \textit{Lernrate} $\eta$ die Schrittweite ist, mit der die
Anpassungen vorgenommen werden.





%------------------- SUBSECTION: Validierung -------------------
\subsection{Validierung und Overfitting}\label{subsec:validation}

Um überprüfen zu können, ob und wie gut ein Modell die Zusammenhänge
in den Trainingsdaten generalisiert hat, dh auch für neue Daten
anwendbar ist,
wird der Datensatz in einen Trainings- und einen Testdatensatz aufgeteilt.

Während des Trainings wird für beide Sätze der Fehler berechnet, 
die korrektur der Gewichte mittels Backpropagation erfolgt
jedoch nur anhand der Trainingsdaten.

Entsteht eine Abweichung der beiden Fehlerfunktion wie in 
\ref{fig:overfitting} dargestellt, findet eine Überanpassung 
(\textit{Overfitting}) des Modells an die Trainingsdaten statt.

\begin{figure}[H]
    \centering
    \def\svgwidth{0.5\textwidth}
    \input{Bilder/Overfitting_svg.pdf_tex}
    \caption{Overfitting, Lossfunktoin, Quelle: \cite{overfittingPlot}}
    \label{fig:overfitting}
\end{figure}

Gründe dafür sind zu wenig Trainingsdaten oder ein zu komplexes Model, 
welches sich aufgrund der vielen Parameter wie bei eine Funktionen hohen
grades, die Möglichkeit hat sich an jeden Datenpunk 
anzupassen und damit nicht mehr generalisierbare Aussagen 
für neue Datenpunkte treffen kann.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{over_under_fit.png}
    \caption{Quelle: \cite{deshpGuideImprovingDeep2017a}}
    \label{fig:over_under_fit}
\end{figure}


Stehen nicht mehr Trainingsdaten zur verfügung, kann Overfitting
mit einer der folgeden Techniken verhindert werden.


\subsubsection{Augmentierung}
Bei Augmentierung werden aus den vorhandenen Daten künstlich mehr 
Daten generiert, in dem an den Bildern geometrische transformationen 
oder manipulationen der pixelwerte vorgenommen werden.

\subsubsection{Regularisierung der Parameter (L1/L2)}
Bei Regularisierung wird der Lossfuction als weiterer Term
eine aufsummierung aller Gewichte hinzugefügt,
wodurch diese bei der Minimierung des Lossfunktion 
klein gehalten werden und damit einhergend weniger potential 
zum Overfitting besteht.

\begin{equation}
    \label{eq:regularization}
    J = L + \lambda \sum_{i} w_{i}^{2}
\end{equation}

\subsubsection{Dropout}
Beim Dropout werden in mit einer bestimmten Wahrscheinlichkeit 
einige Neuronen Neuronen (Bsp 50\%) zu 0 gesetzt, um 
alternative Gewichtsanpassungen zu erzwingen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{dropout.png}
    \caption{Dropout, Quelle: \cite{maksutovDeepStudyNot2018}}
    \label{fig:dropout}
\end{figure}

\subsubsection{Early Stopping}
Beim erly Stopping wird das Traing an der 
Stelle unterbroche, an der die Lossfunktion ihr 
Minimum erreicht hat, markierte stelle in 
\ref{fig:overfitting}.



\section{Computer Vision}

Computer Vision bezeichnet die Erkennung und 
Verarbeitung des Inhalts einer Bild- Oder Video 
Datei. Es kommen dabei Techniken der Bilverarbeitung
zusammen mit Deep Learnig Algorithmen zum Einsatz.


%---------------- SUBSECTION: Convolutional ----------------
\subsection{Convolutional Neural Networks}\label{subsec:cnn}

Convolutional Neural Networks sind eine Erweiterung 
der in \ref{sec:nn} beschriebenen Neuronalen Netze 
und besonders geeignet für die Bilderkennung.
Befor die Klassifizierung mittels Fully Connected Network 
stattfindet, sollen für die Klasse spezifische Merkmale 
aus dem Input Bild heraus extrahiert werden.
Dafür werden über das Bild zeilenweise Filtermatrizen mit kleinerer Dimension
(3x3, 5x5) geschoben und eine math Faltung angewendet.
Die daraus entstehenden Feature Maps sind Matrizen, in denen 
korellationen zwischen fileter ung input in form von 
Mustern abgebildet werden.
Die Werte der Filter Matrizen entsprechen den zu lernenden Gewichten 
und werden mithilfe der Backpropagation angepasst.

\begin{figure}[H]
    \centering
    \label{fig:conv}
    \includegraphics[width=0.4\columnwidth]{convolution.png}
    \caption{Faltung, \cite{researcherSimpleIntroductionConvolutional2019}}
\end{figure}


Durch die hintereinanderschaltung mehrerer Convolutional Layern 
lassen sich so immer komplexere Merkmale des Input Bildes in den 
Feature Maps heraus extrahieren.
Durch Subsampling Methoden wie Max Pool Layer zwischen den Convolutional
Layern verkleinert sich die Dimension der Ferture Maps in jeder Schicht.


\begin{figure}[H]
    \centering
    \label{fig:lenet}
    \includegraphics[width=0.8\columnwidth]{lenet.png}
    \caption{Faltung, \cite{lecunGradientBasedLearningApplied1998}}
\end{figure}


Vorteile der CNNs, verglichen mit den in Abschnitt \ref{subsec:nn} beschiebenen 
\textit{Feed Forward Networks} sind der geringere Rechenaufwand
durch die gemeinsame Nutzung der Paramer durch die Filter Matrizen und die durch die 
Faltung zustande kommende räumliche invarianz für das zu erkennende 
Objekt auf dem Bild.

Um die Features, welche insbesondere in den vordersten Convolutional 
Layern für alle klassen sehr ähnlich sind, nicht bei jedem Modell
neu lernen zu müssen, wird häufig \textit{Transfer Learing} angewendet, 
eine Technik, bei der vortrainierte Filter verwendet und durch Fine 
Tuning an den eigenen Datensatz angepasst werden.


\subsubsection{Architkturen}\label{subsubsec:architecture}

Seit der Einführung des ersten CNN 1998 von Yann LeCun
\cite{lecunGradientBasedLearningApplied1998}, welches in 
Abbildung \ref{fig:lenet} dargestellt ist, wurde eine 
vielzahl an neuen Architekturen für genauere und effizientere
Ergebnisse entwickelt.

Zur bewertung dieser anhand einheitlicher Kriterien,
wurde die \textit{Large Scale Visual Recognition Challenge (ILSVRC)}
\cite{ImageNetLargeScale} von ImageNet gegründet.

Bekannte Modelle, welche die Challenge gewonnen haben 
sind, wie in \cite{StanfordCS231nConvolutional} aufgelistet ist:


\begin{itemize}
    \item Alexnet (2012), mehrere conv layer hintereinander
    \item ZF Net (2013)
    \item GoogleLeNet (2014), Inception Module
    \item VGGNet 2014
    \item ResNet (2015)
\end{itemize}



%---------------- SUBSECTION: Obj Detection ----------------
\subsection{Objekt erkennung}\label{subsec:objdet_det}

Neben der Information, was sich auf einem Bild befindet geht 
es bei der Objekt Erkennung auch darum herausfinden wo sich das 
Objekt auf dem Bild befindet.

Dafür wird die CNN Architektur so erweitert, das das Modell 
Neben den Input Bildern auch die Koordinaten für das 
Training erhällt, und dadurch diese mitlernt.

Dafür gibt es verschiedene Ansätze, welche im Abschnitt \ref{sec:related_work}
genauer beschrieben werden.

\begin{figure}[H]
    \centering
    \label{fig:class_vs_det}
    \includegraphics[width=0.8\textwidth]{classification_detection.jpeg}
    \caption{Unterschied: Classification - Detection}
\end{figure}


%------------------- SUBSECTION: ML Frameworks ---------------
\subsection{Machine Learning Frameworks}

Deep Larning Algorithmen beeinhalten eine vielzahl an komplexen
Berechnungsschritten und Parametern. Um diese diese nicht jedesmal 
von Grund auf neu implementieren zu müssen bieten Deep Learning 
Frameworks eine vereinfachte Möglichkeit die Modelle zu konstruieren.

Einige der bekannten Open Source Frmaworks sind Tensorflow,
Caffe, Torch, Kaldi oder Scikit-Learn.


TensorFlow, welches in der Thesis verwendet wurde, stammt von 
Google und ist ein aufgrund seiner hohen Flexibilität besonders 
in der Forschung oft verwendetes Framework.



%------------------- SECTION: Hardware ----------------------
\section{Neural Compute Stick 2}\label{ncs2}
%noch eine section zu Hardware allg (cpu, gpu, tpu), Neural Compute Stick und AI on the egde

Da das Training und die Inferenz von Deep Learning Algorithmen
 sehr rechenintensiv ist, werden entsprechen leistungsfähige 
Prozessoren benötigt. Dabei ist die Ausführung auf einer GPU 
(Graphical Processor Unit) meist effizienter als auf einer 
CPU (Central Processor Unit).

Anwendungen die auf eingebetteten Systemen laufe, wie etwa 
auf einem Raspberry Pi, kommen dabei schnell an ihre 
Grenzen.

Häufig werden daher die Bilddaten über eine Cloud zur 
verrechnung/inferenz an einen Leistungsstärkeren 
Rechner gesendet.

Sollen die Daten, wie dies beim Edge Computing der Fall ist, 
auf dem gerät direkt verarbeitet werden, gibt es speziell 
für die Inferenz von Deep Learing Algorithmen Ausgelegt 
Hardware, welche NN spezifische Berechnung besonders 
effizient durchführen kann.

Diese können entweder als eigenständiges \textit{System on Chip}
(SoC) System wie zb der \textit{Nvidia Jetson TX2} agieren oder 
zusammen mit einem Host, wie das bei dem in der Arbeit verwendeten 
Neural Compute Stick 2 der fall ist.

Dieser verwendet den Movidius Myriad X Vision Processing Unit (VPU)
welcher neben der Neural Compute Engine zur beschleunigten berechnung 
Neuronalen Netzen, einen Bildbeschleuniger, 16 SHAVE Prozessoren, 
Bildsignalprozessoren udn RISC CPU Core besitzt.
\cite{haussermannFunktionUndEffizienz}
\\[1cm]
\begin{minipage}{0.4\textwidth}
    \centering
    \label{fig:ncs2}
    \includegraphics[width=\textwidth]{ncs2_top.jpg}
    \captionof{figure}{ncsc2}
\end{minipage}
\begin{minipage}{0.6\textwidth}
    \centering
    \label{fig:myriad}
    \includegraphics[width=\textwidth]{myriad.png}
    \captionof{figure}{myriad}
\end{minipage}


\subsection{OpenVino Toolkit}

% doku:
% https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Introduction.html
% diagramme:
% https://github.com/intel-iot-devkit/inference-tutorials-generic/tree/openvino_toolkit_2019_r1_0/car_detection_tutorial#tutorial-step-3-add-the-second-model-vehicle-attributes-detection
% https://github.com/intel-iot-devkit/inference-tutorials-generic/blob/openvino_toolkit_2019_r1_0/face_detection_tutorial/Readme.md


Um die Inferenz eines trainierten Deep Learing Modells auf dem
Neural Compute Stick ausführen zu können, wird das Toolkit 
\textit{OpenVino} verwendet.

Dieses ist eine Plattform zur Otimierung und Inferenz von 
CNN basierten Modellen auf unterschiedlicher Intel Hardware.





Dabei wird ein eigenes Dateiformat verwendet, die \textit{Intermediate 
Representation} (IR), welche die Struktur/Architektur des Modells 
in einer .xml Datei und die trainierten Parameter/Gewichte in 
einer Binary (.bin) datei abbildet.

Mit dem \textit{Model Optimizer} können Modelle der Frameworks 
TensorFlow, Caffe, ONNX, Kaldi, oder MXNET in das IR Format 
konvertiert werden.

Um diese dann auf die entsprechende Hardware zu laden und anwendbar 
zu machen, wird die auch in OpenVino enthaltene
\textit{InferenceEngeine} verwendet.

Diese bietet eine Api mit der aus der Anwendung heraus in den 
Programmiersprachen C++ oder Python auf die Funktionen der 
InferenceEngeine zugegriffen werden können.



\begin{figure}[H]
    \centering
    \def\svgwidth{0.8\textwidth}
    \input{Bilder/open_vino_workflow_neu.pdf_tex}
    \caption{OpenVino Workflow}
    \label{fig:openvinoflow}
\end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{./Bilder/open_vino_workflow_steps.png}
%     \caption{Workflow: OpenVino Toolkit}
%     \label{img:openvinoworkflow}
% \end{figure}

