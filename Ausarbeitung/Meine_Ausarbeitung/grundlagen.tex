\chapter{Grundlagen}\label{kap:grundlagen}

%####################  CHAPTER 1: Grundlagen  #################

In vorliegendem Kapitel werden zunächst die Grundlagen 
des Machine Learnings, mit künstlichen Neuronalen Netzen, 
beschrieben. Der zweite Abschnitt wird die, in der 
Bilderkennung eingesetzen, \textit{Convolutional Neural 
Networks} näher erläutern.
Im letzten Abschnitt wird die, für die Inferenz, 
verwendete Hardware, der \textit{Neural Compute Stick 2}
beschrieben werden.

%------------------- SECTION: Machine Learning ------------------

\section{Machine Learning}\label{sec:ml}

Beim Machine Lerining, welches ein Teilgebiet der Computerwissenschafen
ist, geht es um die Erstellung von Algorithmen, die Zusammenhänge in 
großen Datenmengen erkennen können, ohne explizit darauf programmiert
worden zu sein.
Eine Form davon ist das \textit{Supervised Learning}, bei dem das Programm 
neben den Input Daten auch die Zugehörigen Ausgaben, in vorm von 
Labels, erhält, um daraus 
Regeln für die Zusammenhänge zwischen Ein- und Ausgabe Daten abzuleiten.
Darin unterscheidet sich das Vorgehen wesentlich zur Programmierung 
eines klassischen Programms, bei dem die Regeln vorarb definiert 
werden müssen, wie folgende Grafik veranschaulichen soll.
\vspace{1cm}
\begin{figure}[H]
    \centering
    \input{Bilder/ml_classic_system.tex}
\end{figure}
\vspace{1cm}
Das Ableiten der Regeln erfolgt beim Machine Learning in einem 
iterativen Prozess, welcher als Training bezeichnet wird.
Dabei werden die Zusammenhänge zwischen Ein- und Ausgabe Daten 
als mathematische Funktion, die numerisch 
angenähert wird, betrachtet.

Handelt es sich um einen linearen Zusammenhang der 
Daten, spricht man von einem \textit{Regression}, 
wohingegen bei einer Kategorisierung von diskreten 
Werten von einer \textit{Klassifikation} gesprochen wird.

Weitere Formen neben dem \textit{Supervised Learning} sind das 
\textit{Unsupervised Learning}, bei dem das Programm keine Labels 
erhält, sondern diese durch das Training erst finden 
soll, oder das \textit{Reinfocement Learning}, bei dem das Programm 
durch interaktion mit der Umwelt bestimmte Verhaltensmuster lernt.
Da in der Bachelor Arbeit nur mit dem Supervised Learning 
gearbeitet wurde, wird auf diese Techniken im folgenden nicht 
näher eingegangen.

%------------------- SECTION: Neuronale Netze -------------------

%\newpage
\subsection{Künstliche Neuronale Netze} \label{subsec:nn}

Für komplexe Input Daten, wie beispielsweise Bilder, bei denen 
die einzelnen Pixelwerte die Eingaben und der Inhalt des Bildes die 
gesuchte Ausgabe darstellen, werden meistens künstliche Neuronale
Netze verwendet.
Diese sind eine Form des Machine Learings und bestehen aus einer 
vielzahl an, programmatisch erzeugten, künstlicher Neuronen, die 
in Schichten angeordnet, miteinander verbunden sind.

Durch unterschiedlich starke Gewichtungen der einzelnen
Verbindungen, welche auch als Gewichte bezeichnet werden, 
können, wie in Abbildung \ref{fig:nn} schematisch dargestellt ist,
zu gegebene Eingabe Daten die richtigen Ausgaben 
gefunden werden.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \def\svgwidth{0.85\columnwidth}
    %\footnotesize
    \input{Bilder/nn.pdf_tex}
    \caption{Vereinfachte Darstellung eines Künstlichen 
    Neuronalen Netzes}
    \label{fig:nn}
\end{figure}
\vspace{1cm}

Die richtigen Einstellungen der Gewichte erfolgt dabei
in dem iterativen Trainingsprozess, welcher aus folgenden 
drei schritten besteht und in Abbildung \ref{fig:train}
schematisch dargestellt ist.

\begin{itemize}
    \item \textit{Forward Pass} Für die Inputdaten anhand,
            aktueller Gewichte, eine Schätzung für die Ausgabe treffen
    \item \textit{Fehlerbestimmung} Abweichung der gemachten Schätzung,
             zum tatsächlichen Wert, berechnen
    \item \textit{Backpropagation} Minimierung der
            Abweichung, durch Justierung der Gewichte
\end{itemize}

\vspace{1cm}
\begin{figure}[H]
    \centering
    \input{Bilder/train_workflow.tex}
    \caption{Trainingsablauf eines Neuronalen Netzes}
    \label{fig:train}
\end{figure}
\vspace{1cm}


Durch mehrfaches Durchlaufen dieser Schritte, 
kann die Fehlerfunktion soweit minimiert werden, dass 
das Modell auch für neue, ungesehene Input Daten 
richtigen Vorhersagen treffen kann.

Die Funktionsweisen der drei Schritte 
werden im folgenden näher erklärt.

\subsubsection{Forward Pass}
Im \textit{Forward Pass} werden die Inputs, welche an der 
ersten Schicht aus Neuronen anliegen, durch alle Schichten hindurch 
gereicht, um in der Ausgabeschicht Schicht
das gesuchte Ergebnis zu liefern.
Dabei erhält jedes Neuron die mit den Parametern $w_{i}$ gewichteten
Ausgabewerte aller Neuronen der vorherigen Schicht und summiert diese
zusammen mit einem konstanten Bias Wert $b$, als Offset, auf.

Mithilfe einer Aktivierungsfunktion wird der Wert, wie 
in Abbildung \ref{fig:neuron} dargestellt ist, auf
einen bestimmten Bereich skaliert.


\vspace{1cm}
\begin{figure}[H]
    \centering
    \input{Bilder/neuron}
    \caption{Berechnungen an einem einzelnen Neuron}
    \label{fig:neuron}
\end{figure}
\vspace{1cm}

Um diesen Vorgang für eine gesamte Schicht, bestehend aus 
einer Vielzahl an Neuronen, zu berechnen, werden die Schichten 
als Vektoren $x$ und die Gewichte als Matrizen $W$ dargestellt.

Durch Bilden der Matrixmultiplikation aus $x$ und $W$, wie Gleichung 
\ref{eq:forward} zeigt,
erhält man den \textit{Forward Pass} von einer Schicht zur nächsten.

\vspace{0.5cm}
\begin{equation}
    \label{eq:forward}
    z = W^{T}x+b
\end{equation}
\vspace{0.5cm}

Der resultierende Vektor $z$ wird dann elementweise
einer nichtlinearen Aktivierungsfunktion $g(z)$ übergeben.

Bei dieser handelt es sich, für die mittleren Schichten, 
den sogenannten \textit{Hidden Layer},  
meist um die im Plot \ref{plot:relu} dargestellte \textit{ReLU} Funkton,
welche positive Werte beibehält und negative 
Werte zu 0 setzt.

In der letzten Schicht, welche die Wahrscheinlichkeiten 
für mögliche Ausgaben enthält, wird eine Aktivierungsfunktion
verwendet, die den Wert zwischen 0 und 1 
skaliert.
Dabei wird für eine binären Klassifikation die 
im Plot \ref{plot:sigmoid} dargestellte \textit{Sigmoid} 
Funktion verwendet, welche die Werte S-Förmig zwischen 
0 und 1 Skalliert.

Für eine kategorische Klassifikation, 
mit mehr als zwei möglichen Ausgabewerten, 
wird die in Gleichung 
\ref{eq:softmax} dargestellte \textit{Softmax} Funktion 
verwendet, welche 
eine Wahrscheinlichkeitsverteilung
über alle Werte der Ausgebeschicht generiert.

\begin{equation}
    \label{eq:softmax}
    g(z) = \frac{e^{z}}{\sum e^{x}}
\end{equation}
\newpage
%\vspace{1cm}
\begin{minipage}{0.5\textwidth}
    \centering
    \begin{equation*}
        \label{eq:relu}
        g(z) = max\{0,z\}
    \end{equation*}
\end{minipage}
\vspace{1cm}
\begin{minipage}{0.5\textwidth}
    \centering
    \begin{equation*}
        \label{eq:sidmoid}
        g(z) = \frac{1}{1 + e^{-x}}
    \end{equation*}    
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \input{Bilder/relu.tex}
    \captionof{figure}{ReLU Funktion} 
    \label{plot:relu}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \input{Bilder/sigmoid.tex}
    \captionof{figure}{Sigmoid Funktion} 
    \label{plot:sigmoid}
\end{minipage}
\vspace{1cm}



\subsubsection{Fehlerbestimmung}
Die Abweichung des geschätzten Wertes, welcher an den Neuronen
der letzten Schicht anliegt, zum tatsächlichen Wert,
wird mithilfe einer geeigneten Fehler- oder Lossfunktion ermittelt.
Ein Regressionsmodell verwendet hierbei oft 
den absoluten oder quadratischen Abstand der beiden 
Werte, wohingegen für Klassifikationsmodelle meist 
der Logarithmus verwendet wird.
In Gleichung \ref{eq:crossentropy} ist die logarithmische, sogenannte 
\textit{Cross enropy} Lossfunkton für eine Binäre Klassifikation 
dargestellt.
Durch den Logarithmus wird der Loss um so größer,
je weiter die Schätzung $y$ vom 
tatsächlichen Wert $\hat{y}$ abweicht.
\vspace{0.5cm}
\begin{equation}
    \label{eq:crossentropy}
    L = \hat{y}log(y) + (1 - \hat{y})log(1 - y)
\end{equation}
\vspace{0.5cm}

\subsubsection{Backpropagation}

Die Anpassung der Gewichte, zur Minimierung der Lossfuntion, 
kann durch Berechnung des Gradienten dieser, erfolgen.

Dafür wird die die Fehlerfunktion $L$, für jede Schicht, partiell
nach den Gewichten $w$ der Schicht abgeleitet,
was, wie in Gleichung \ref{eq:backprop}
dargestellt, mithilfe der Kettenregel geschieht.
Mit dem ermittelten Gradienten werden
dann die Gewichte nach Gleichung \ref{eq:update_wieghts}
mit einer Schrittweite $\eta$ angepasst.

\vspace{0.5cm}
\begin{equation}
    \label{eq:backprop}
    \frac{\partial L}{\partial w} = \frac{\partial L}{\partial z}\frac{\partial z}{\partial w}
\end{equation}
\vspace{0.5cm}
\begin{equation}
    \label{eq:update_wieghts}
    w  \leftarrow w - \eta \frac{\partial L}{\partial w}
\end{equation}
\vspace{1cm}



%------------------- SUBSECTION: Validierung -------------------
\subsection{Validierung und Overfitting}\label{subsec:validation}

Um überprüfen zu können, ob und wie gut, ein Modell die Zusammenhänge
in den Trainingsdaten generalisiert hat, also auch für neue Daten
anwendbar ist,
wird der Datensatz in einen Trainings- und
einen Testdatensatz aufgeteilt.

Die Abweichung wird, während des Trainings, für beide Datensetze 
berechnet, die Korrektur der Gewichte, mittels Backpropagation, erfolgt
jedoch nur anhand der Trainingsdaten.

Indem beide Lossfunktionen als Funktion in Abhängigkeit 
der Iterationen geplottet werden, kann eine Überanpassung
des Modells an die Trainingsdaten, festgestellt werden.

Dabei handelt es sich um das sogenannte \textit{Overfitting}
was daran zu erkennen ist, das sich nur noch der Losswert der 
Trainingsdaten verringert und der der Testdaten 
gleich bleibt oder sich wie in Abbildung \ref{fig:overfitting}
dargestellt, wieder verschlechtert.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \def\svgwidth{0.5\textwidth}
    \input{Bilder/overfitting.pdf_tex}
    \caption{Overfitting, anhand der Losskurven}
    \label{fig:overfitting}
\end{figure}
\vspace{1cm}

Gründe für Overfitting können sein, dass zu wenig Trainingsdaten
verwendet wurden, oder ein für den Anwendungsfall 
zu komplexes Modell gewählt wurde.

Durch die Überparametrisierung eines zu komplexen Modells hat 
dieses die Möglichkeit sich an jeden Trainingsdatenpunkt einzelnd
anzupassen, sodass keine generalisierbaren 
Aussagen für neue Datenpunkte mehr getroffen werden können.


Der andere Extremfall ist das \textit{Underfitting}, 
bei dem das Modell, aufgrund zu weniger Parameter, nicht die 
Möglichkeit hat, sich an die Trainingsdaten anzunähern.

Die Plots in Abbildung \ref{fig:over_under_fit} veranschaulichen 
die drei Fälle anhand einer polynomialen Funktionen,
die sich an gegebene Datenpunkte, mit unterschiedlech hohem 
Grad, annähern soll.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \def\svgwidth{\textwidth}
    \input{Bilder/over_under_fit.pdf_tex}
    \caption{Annäherung unterschiedlich komplexer Modelle an die gleichen 
        Datenpunkte}
    \label{fig:over_under_fit}
\end{figure}
\vspace{1cm}

Das Auftreten von Overfitting kann entweder durch 
Verwendung einer größeren Anzahl an Trainingsdaten, 
oder mit einer der Folgenden Techniken, vermieden werden.


\subsubsection{Augmentierung}

\textit{Augmentierung} der Daten ist eine Effektive Technik 
gegen Overfitting, bei der künstlich,
aus den vorhandenen Daten, mehr Daten generiert werden. 
Im Fall der Bilderkennung werden hierbei 
die Inputbilder leicht abgeändert, indem z.B. geometrische 
Transformationen oder manipulationen der Farbwerte 
vorgenommen werden.


\subsubsection{Regularisierung der Parameter}

Bei der \textit{Regularisierung} wird der Lossfunktion, als weiterer Term,
eine Aufsummierung aller Gewichte hinzugefügt.

Die Minimierung der Lossfunktion hat dann auf 
die Gewichte den Effekt, dass diese möglichst 
kleine Werte behalten, wodurch das Modell weniger
Möglichkeit zur Überanpassung hat.


Dabei wird zwischen der \textit{L1 Regulierung}, mit einer
absoluten, und der in Gleichung \ref{eq:regularization}
dargestellten \textit{L2 Regularisierung}, mit einer 
quadratischen Aufsummierung der
Gewichte unterschieden.

\begin{equation}
    \label{eq:regularization}
    J = L + \lambda \sum_{i} w_{i}^{2}
\end{equation}


\subsubsection{Dropout}
\textit{Dropout} ist eine Technik, bei der in einigen Schichten 
mit einer bestimmten Wahrscheinlichkeit Werte von 
Neuronen zu 0 gesetzt werden.
Dadurch wird das Modell gezwungen alternative
Gewichtsanpassungen zu finden.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{dropout.png}
    \caption{Dropout, Quelle: \cite{maksutovDeepStudyNot2018}}
    \label{fig:dropout}
\end{figure}


\subsubsection{Early Stopping}
Beim \textit{Early Stopping} wird das Training, 
bevor Overfitting stattfinden kann, abgebrochen.
Das ist, wie in Abbildung \ref{fig:overfitting}
markiert, durch das Minimum der Losskurve der Testdaten 
definiert.



%------------------- SUBSECTION: ML Frameworks ---------------
\subsection{Machine Learning Frameworks}

Machine Learning Algorithmen beinhalten eine vielzahl an komplexen
Berechnungsschritten und Parametern. Um diese nicht jedesmal 
von Grund auf neu implementieren zu müssen bieten 
Frameworks eine einfache Möglichkeit die Modelle zu konstruieren.

Einige der bekannten Open Source Frameworks sind Tensorflow,
Caffe, Torch, Kaldi oder Scikit-Learn.

Für die Bachelor Arbeit wurde Tensorflow verwendet,
ein von Google stammendes Framework,
welches aufgrund seiner hohen Flexibilität besonders 
in der Forschung oft verwendet wird.



%---------------- SUBSECTION: Convolutional ----------------
\section{Convolutional Neural Networks}\label{subsec:cnn}

Convolutional Neural Networks (CNNs) erweitern 
die in Abschnitt \ref{subsec:nn} beschriebenen
\textit{Feedforward neural Networks} um zusätzliche Schichten,
die vor der eigentliche Klassifikation ausgeführt werden
und Merkmale aus den Input Daten herausextrahieren.
Diese Schichten erhalten die Input Daten als
zweidimensionale Matrix und führen darauf 
mathematische Faltungsoperationen aus.
CNNs kommen größtenteils in der Bilderkennung zum 
Einsatz, weitere Anwendungsgebiete sind 
z.B. die Spracherkennung.

Anstatt das alle Neuronen zweier benachbarter Schichten 
durch gewichtete Parameter miteinander verbunden sind, 
stellen kleinere, sogenannte Filter Matrizen, die 
Parameter dar.
Diese werden zeilenweise über das Input Bild 
geschoben, wobei an jeder Stelle eine mathematische 
Faltung mit dem überlappten Bereich des Inputs 
durchgeführt wird. In Abbildung \ref{fig:faltung}
ist dieser Vorgang veranschaulicht.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{convolution.png}
    \end{minipage}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{convolution-layer-a.png}
    \end{minipage}  
    \caption{Faltung des Inputs mit einer Filter Matrix,
    Quellen: \cite{researcherSimpleIntroductionConvolutional2019}
    und \cite{amidiSuperVIPCheatsheet2019}}
    \label{fig:faltung}
\end{figure}
\vspace{1cm}

Jedes Faltungsergebnis, ergibt einen Wert für 
die nächste Schicht, die dadurch 
ein Korrelationsverhältnis zwischen
Filter Matrix und Input Bild erhält.
So werden, in den Filtern definierte Muster, wie 
beispielsweise vertikale Linien, 
aus dem Inputbild herausextrahiert und in 
den Folgeschichten, als sogenannte Feature Maps
abgebildet.


\vspace{1cm}
\begin{equation}
    \label{eq:faltung}
    \begin{pmatrix}
        10 & 10 & 10 & 0 & 0 & 0\\
        10 & 10 & 10 & 0 & 0 & 0\\
        10 & 10 & 10 & 0 & 0 & 0\\
        10 & 10 & 10 & 0 & 0 & 0\\
        10 & 10 & 10 & 0 & 0 & 0\\
        10 & 10 & 10 & 0 & 0 & 0
    \end{pmatrix}
    \times
    \begin{pmatrix}
        1 & 0 & -1\\
        1 & 0 & -1\\
        1 & 0 & -1
    \end{pmatrix}
    = 
    \begin{pmatrix}
        0 & 30 & 30 & 0\\
        0 & 30 & 30 & 0\\
        0 & 30 & 30 & 0\\
        0 & 30 & 30 & 0
    \end{pmatrix}
\end{equation}
\vspace{0.5cm}
\begin{figure}[H]
    \centering
    \def\svgwidth{0.6\textwidth}
    \input{Bilder/convolution_graphical_all.pdf_tex}
    \caption{}
    \label{fig:faltung3}
\end{figure}

In Gleichung \ref{eq:faltung} ist beispielhaft die Faltung 
eines Input Bildes mit einem Filter, zur Erkennung 
vertikaler Linien, dargestellt. Da pro Zeile 
vier Faltungen angewendet werden, entsteht 
eine $4\times4$ Matrix. Soll die Ausgangsgröße 
(hier $6\times6$) beibehalten werden, kann
\textit{zero padding} verwendet werden.

Durch die Faltung entsteht eine räumliche 
Invarianz für das zu erkennende Objekt im 
Input Bild.

Den \textit{Convolutional Layern} folgen meist \textit{Pooling Layer}, 
zum Downsampling, und eine ReLU-Aktivierungsfunktion.

Beim Pooling wird eine bestimme Anzahl an Werten 
zusammengefasst, indem entweder das Maximum oder der 
Mittelewert dieser Werte verwendet werden.

Durch hintereinanderschaltung mehrerer solcher Convolutional Blöcke,
können in jeder Schicht immer komplexere Muster aus dem 
Input Bild herausextrahiert werden.

Die Fetures des Letzten Convolutional Layers werden dann einem 
\textit{Fully Connected Layer}  zur Klassifikation 
übergeben, wie in Abbildung \ref{fig:lenet} anhand des 
ersten, von Yann LeCun entworfenen, Convolutional Neural 
Networks zu erkennen ist.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{lenet.png}
    \caption{LeNet-5 Architektur
    \cite{lecunGradientBasedLearningApplied1998}}
    \label{fig:lenet}
\end{figure}
\vspace{1cm}

Ein wesentlicher Vorteil gegenüber einem reinen 
\textit{Feedforward Network} ist, dass 
durch die gemeinsame Nutzung von Parametern, 
durch die Filter, ein geringerer Rechenaufwand entsteht.

Die Werte der Filter Matrizen, welche die zu 
extrahierenden Muster darstellen, 
werden über die Backpropagation eingelernt.

Da die Merkmale, insbesondere in den vorderen Convolutional 
Layern, für die meisten Klassen sehr ähnlich sind,
werden häufig Modelle mit vortrainierten Filtern 
verwendet.

Durch das sogenannte \textit{Transfer Learning}
müssen die Gewichte dann nur noch leicht, 
für den eigenen Datensatz, angepasst werden.



\subsection{Architkturen}\label{subsubsec:architectures}

Nachdem 1998 das erste CNN (Abbildung \ref{fig:lenet})
 von Yann LeCun in 
\cite{lecunGradientBasedLearningApplied1998} 
vorgestellt wurde, gab es eine vielzahl 
an Weiterentwicklungen, welche genauere und 
effizientere Modelle hervorbracheten.

Gemessen und verglichen werden die Ergebnisse häufig an 
der \textit{Large Scale Visual Recognition Challenge (ILSVRC)}
\cite{ImageNetLargeScale}.
 
Namhafte Modelle, welche die Challenge in den letzten 
Jahren gewinnen konnten sind unter \cite{stanfordConvNetList}
zu finden und im folgenden aufgelistet.

\begin{itemize}
    \item \textbf{AlexNet}, (2012), von Alex Krizhevsky 
        \cite{krizhevskyImageNetClassificationDeep2017b} besitzt eine 
        ähnliche Struktur wie LeCuns LeNet, ist jedoch Tiefer und 
        hat mehrere Convolutional Layer am Stück hintereinander,
        wodurch die Genauigkeit erhöht wurde.

    \item \textbf{ZF Net}, (2013), von Matthew Zeiler and Rob Fergus,
        \cite{zeilerVisualizingUnderstandingConvolutional2013}
        konnte das AlexNet durch eine Vergrößerung der 
        mittleren Convolutional Layer und eine Verkleinerung 
        der Filter in den vorderen Schichten weiter optimieren.

    \item \textbf{VGGNet}, (2014), von Karen Simonyan and Andrew Zisserman
        \cite{simonyanVeryDeepConvolutional2015}.
        Dieses Modell zeigte, dass ein tieferes Netz (16 bis 19 ConvLayer)
        mit reduzierter Filter größe ($3\times3$) bessere Ergebnisse erzielt.    

    \item \textbf{GoogleLeNet}, auch bekannt als Inception, (2014),
        von Szegedy et al \cite{szegedyGoingDeeperConvolutions2014},
        konnte mit den Inception Modulen,
        welche im folgenden genauer erläutert werden, die Zahl der 
        Parameter, und dadurch den Rechenaufwand, deutlich veringern.

    \item \textbf{ResNet}, (2015) von Kaiming He et al 
        \cite{heDeepResidualLearning2015}, enthällt 
        als Erweiterung die sog. \textit{Residual Blocks}, in denen auf
        das Ergebnis eines Bocks zusätzlich der unveränderte
        Input Wert addiert wird.

\end{itemize}



\subsubsection{GoogleLeNet (Inception)}

Die Entwicklung der CNN Architekturen hat gezeigt, dass 
sich durch Hinzufügen weiterer Schichten, sowie
der Verwendung einer größeren Anzahl an Neuronen je Schicht, 
die Genauigkeit verbessern lässt.
Das bringt jedoch auch die Nachteile eines 
größeren Rechenaufwands sowie der erhöhten 
Gefahr des Overfittings mit sich.

Das in \cite{szegedyGoingDeeperConvolutions2014} 
vorgestellte GoogleLeNet, hat mit den in 
Abbildung \ref{fig:incept_modul} dargestellten
\textit{Inception Modulen}, einen neuen, 
effizienteren, Asatz gefunden, die 
Komplexität und damit die Genauigkeit eines 
CNNs zu erhöhen.

Die Module bestehen aus parallel ausgeführten 
Convolutional Layern der unterschiedlichen Filter 
Größen $1\times1$, $3\times3$ und $5\times5$ welche 
am ende des Moduls über eine \textit{Filter 
concatenation} wieder zusammengeführt werden.
Zur Dimensionsreduktion werden, wie in 
Abbildung \ref{fig:incept_modul} dargestellt,
diesen Filtern noch $1\times1$ Filter vorgeschaltet.
Durch die Inception Module kommt das Modell, 
für gleiche Ergebnisse, mit deutlich weniger 
Parametern aus, als ein Modell ohne die Module.
Ein weiterer Vorteil ist, das durch die 
unterschiedlichen Filtergrößen, Merkmale 
unterschiedlicher skalierungen besser gefunden 
werden können.

Um die Effizienz weiter zu Steigern wurden in 
der zweiten Version des GoogleLeNet, beschrieben in
\cite{szegedyRethinkingInceptionArchitecture2015},
neben anderen Verbesserungen, die 
$5\times5$ Filter, jeweils durch zwei $3\times3$ Filter, 
ersetzt, was in Abbildung \ref{fig:incept_modul2}
dargestellt ist.

\vspace{1cm}
\begin{minipage}{0.45\textwidth}
    \centering
    \input{Bilder/inception_module.tex}
    \captionof{figure}{Inception Module Version 1}
    \label{fig:incept_modul}
\end{minipage}
\begin{minipage}{0.1\textwidth}
    \hfill
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \centering
    \input{Bilder/inception_module_v2.tex}
    \captionof{figure}{Inception Module Version 2}
    \label{fig:incept_modul2}
\end{minipage}


\newpage
\subsubsection{Mobilenet}
Das MobileNet \cite{howardMobileNetsEfficientConvolutional2017a}
wurde mit dem Ziel geschaffen, durch eine geringere 
komplexität, für Mobile Endgeräte oder Embedded Anwendungen 
geeignet zu sein.

Dafür wurden die üblichen \textit{full convolutional Layer}
mit sogenannten \textit{Depthwise Seperable 
Convolutions} ersetzt, welche die Faltung in zwei seperaten 
Layern ausführt. Zuerst wird eine \textit{Depthwise  
Convolutions} auf die drei Farbchannel getrennt ausgeführt.
Anschleißend führt eine \textit{pointwise convolution}
mit $1\times1$ Filter diese wieder zusammen.



In der zweiten version des MobileNet
\cite{sandlerMobileNetV2InvertedResiduals2019},
wurden, die \textit{Depth-wise Separable Convolutions}
wie folgt abgeändert:

Zuerst wird eine $1\times1$ Convolution mit ReLU
Aktivierungsfunktion ausgefühtr, anschließend die 
\textit{Depthwise Convolutions}, gefolgt 
von einer weiteren $1\times1$ mit linearer 
Aktivierungsfunktion.


Desweiteren soll wie beim \textit{ResNet} eine 
\textit{residual connection}, welche Ein- mit Ausgabe 
eines Blocks verbindet, 
den Gradientenfluss unterstützen, wie in Abbildung 
\ref{fig:mobilenetv2} dargestellt ist.
\vspace{1cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{mobilenet_v2.png}
    \caption{Residual block des MobilenetV2,
    Quelle: \cite{mobilenetv2Bild}}
    \label{fig:mobilenetv2}
\end{figure}


%---------------- SUBSECTION: Obj Detection ----------------
\subsection{Objekterkennung}\label{subsec:objdet_det}

Neben der Information, was sich auf einem Bild befindet, 
soll bei der Objekterkennung zusätzlich herausgefunden werden, 
wo sich das erkannte Objekt auf dem Bild befindet.
In Abbildung \ref{fig:class_vs_det} wird der Unterschid 
veranschaulicht. Im linken Bild (Klassifikation) 
reicht es aus, dass das Modell das Vorhandensein einer Katze
im Bild, mit einer bestimmten Wahrscheinlichkeit, Schätzen kann,
im rechten Bild (Objekterkennung), soll das Modell, 
in Form von \textit{Bounding Boxen}, auch eine 
Lokalisierung vornehmen.

\vspace{1cm}
\begin{minipage}{0.5\textwidth}
    \centering
    Classification
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    Detection
\end{minipage}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{classification_detection.jpeg}
    \caption{Unterschied: Klassifikation - Objekterkennung, 
    Quelle: \cite{ouaknineReviewDeepLearning2018a}}
    \label{fig:class_vs_det}
\end{figure}
\vspace{1cm}
Dafür wird die CNN Architektur so erweitert, dass dem 
Modell für das Training neben den Klassenlabels auch
die Koordinaten der Bounding Boxen, welche das Objekt 
umramen, mit übergeben werden.

Diese können dann, mittels Regressionsverfahren, 
durch Annäherung der geschätzen an die richtigen 
Koordinaten, gelernt werden.

Bei dem Verfahren zur Objekterkennung gibt es 
verschiedene Ansätze, die alle ein Basis-CNN
zur \textit{Feature Extraction} verwenden.
Die Lokalisierung findet über eine 
Vorschlagsgenerierung statt, welche aus 
Regionen im Input Bild besteht, die 
am wahrscheinlichsten ein Objekt enthalten.

Die Generierung der Regionen kann z.B. durch
\textit{Selective Search} oder
seperatem \textit{Region Proposial Network}
stattfinden.



%------------------- SECTION: Hardware ----------------------
\section{Neural Compute Stick 2}\label{ncs2}

Da das Training und die Inferenz von Deep Learning Algorithmen
sehr rechenintensiv ist, werden entsprechen leistungsfähige 
Prozessoren benötigt. Dabei ist die Ausführung auf einer GPU 
(Graphical Processor Unit) meist effizienter als auf einer 
CPU (Central Processor Unit).

Anwendungen die auf eingebetteten Systemen oder einplatinen 
Computern wie dem Raspberry Pi laufen, kommen dabei schnell
an die Grenzen.

Eine Möglichkeit, dieses Problem zu umgehen,
ist es, die Bilddaten für die 
Verrechnung an eine Cloud zu senden, wo sie 
von einem leistungsstärkeren Rechner inferiert und 
dann wieder zurückgesendet werden.

Sollen die Daten, wie es beim Edge Computing der Fall ist, 
auf dem Anwendungsgerät direkt verarbeitet werden,
gibt es speziell für die Inferenz von Deep Learing Algorithmen
geeignete Hardware.
Durch Fokus auf hohe Paralletlität anstatt schneller Taktrate
bei den Berechnungen, können solche Prozessoren
Deep Learning spezifische Rechenoperationen, 
wie z.B. der Matrixmultiplikation, besonders effizient 
ausführen.

Die Inferenzbeschleunigende Hardware kann dabei entweder
als eigenständiges \textit{System on Chip (SoC)}
System wie z.B. der \textit{Nvidia Jetson TX2} agieren, oder
in Verbindung mit einem Host Pc, wie der, in der Arbeit 
verwendete, Neural Compute Stick 2 von Intel.

Der in Abbildung \ref{fig:ncs2} gezeigte Neural Compute 
Stick 2 (NCS2) verwendet für die Inferenz eine
Movidius Myriad X Vision Processing Unit (VPU),
welche in Abbildung \ref{fig:myriad} schematisch 
dargestellt ist.

Diese besteht, wie in
\cite{haussermannFunktionUndEffizienz} genauer 
beschrieben ist, aus der Neural Compute Engine, 
zur beschleunigten Berechnung Neuronaler Netze,
einem Bildbeschleuniger, 16 SHAVE Prozessoren, einem 
Bildsignalprozessor sowie einem RISC CPU Core.

\vspace{1cm}
\begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{ncs2_top.jpg}
    \captionof{figure}{NCS2,
    Quelle: \cite{ncs2}}
    \label{fig:ncs2}
\end{minipage}
\begin{minipage}{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{myriad.png}
    \captionof{figure}{Myriad Chip,
    Quelle: \cite{IntelMyriadVision}}
    \label{fig:myriad}
\end{minipage}


\subsection{OpenVino Toolkit}

Zur Ausführung der Inferenz, eines trainierten Deep Learing
Modells, auf dem Neural Compute Stick, wird das Toolkit 
\textit{OpenVino} von Intel verwendet.
Dieses ist eine Software Plattform zur Otimierung und Inferenz 
von CNN basierten Modellen auf verschiedener Intel Hardware.

Dabei wird ein eigenes Dateiformat für die Modelle verwendet, 
die \textit{Intermediate Representation} (IR),
welche die Struktur des Modells 
in einer Xml-Datei (.xml) und die trainierten Gewichte in 
einer Binary Datei (.bin) definiert.
Mit dem \textit{Model Optimizer} des Toolkits,
können Modelle welche in den den Frameworks TensorFlow,
Caffe, ONNX, Kaldi, oder MXNET trainiert wurden, 
in das IR Format konvertiert werden.

Um diese dann auf die entsprechende Hardware zu laden und anwendbar 
zu machen, wird die auch in OpenVino enthaltene
\textit{InferenceEngine} verwendet.
Diese bietet eine Api, mit der aus der Anwendung heraus, in den
Programmiersprachen C++ oder Python, auf die Funktionen der 
InferenceEngine zugegriffen werden können.


In Abbildung \ref{fig:openvinoflow} ist der Workflow mit 
Openvino, welcher das Training eines Deep Learning Modells 
mit der Implementierung einer Nutzer Anwendung verbindet, 
dargestellt.


\vspace{1cm}
\begin{figure}[H]
    \centering
    \def\svgwidth{0.8\textwidth}
    \input{Bilder/open_vino_workflow_neu.pdf_tex}
    \caption{OpenVino Workflow, angelent an 
    \cite{openvinoflow}}
    \label{fig:openvinoflow}
\end{figure}
