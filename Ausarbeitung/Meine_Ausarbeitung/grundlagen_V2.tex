\chapter{Grundlagen}\label{kap:grundlagen}

%####################  CHAPTER 1: Grundlagen  #################

Im folgende Kapitel wird zunächst auf die Grundlegenden Techniken
 des Machine Learings, insbesondere auf die für die Bilderkennung 
verwendeten Convolutional Neural Networks eingegangen.

Anschließend wird es um die verwendete Hardware, den Neural 
Compute Stick 2 un seine Anwendungen gehen.


%------------------- SECTION: Machine Learning ------------------

\section{Machine Learning}\label{sec:ml}

Beim Machine Lerining, welches ein Teilgebiet der Computerwissenschafen
ist, geht es um Algorithmen, die Zusammenhänge in großen Datenmengen 
erkennen sollen, ohne expliziet darauf programmiert worden zu sein.

Eine Form davon ist das \textit{Supervised Learning}, bei der das Programm 
neben den Input Daten auch die Zugehörigen Ausgaben erhällt und daraus 
dann die Regeln für Zusammenhänge herleiten soll.
Dadurch unterscheidet sich das Vorgehen wesentlich zur klassischen Programmierung,
bei bei der die Regeln vorab definiert werden.

\vspace{0.5cm}
\begin{figure}[htb]
    \centering
    \def\svgwidth{0.8\columnwidth}
    \footnotesize
    \input{Bilder/ml_classic_system.pdf_tex}
\end{figure}
\vspace{0.5cm}

Das herleiten der Regeln erfolgt beim Machine Learning dabei in einem 
iterativen Prozess, welcher als Training bezeichnet wird.
Dabei soll eine math. Funktion, welche die Zusammenhänge beschreibt 
numerisch angenähert werden. Ist der Zusammenhang linear, spricht man 
von einer Regrassion, handelt es sich um Kategorische, liegt 
ein Klassifizierung problem vor.
\\
Weitere Formen neben dem \textit{Supervised Learning} sind das 
\textit{Unsupervised Learning}, bei der das Programm keine Labels 
erhällt, sondern diese durch Clustering Verfahren selber finden 
soll, oder das \textit{Reinfocement Learning}, bei dem das Programm 
mit der Umwelt interagieren soll.
\\
Da hier jedoch ausschließlich mit dem Supervised Learing gearbeitet
wurde, werden diese Techniken nicht näher erläutert.


%------------------- SECTION: Neuronale Netze -------------------

\subsection{Künstliche Neuronale Netze} \label{sec:nn}

Für komplexe Input Daten, wie beispielsweise Bilder, bei denen 
die einzelnen Pixelwerte als Inputs und der Inhalt des Bildes als 
Output dienen, werden in der Regel künstliche Neuronale Netze verwendet.
Diese sind eine Form des Machine Learings und bestehen aus einer 
vielzahl an miteinander verbundener Neuronen. Durch unterschiedlich 
starke Gewichtungen der einzelnen Verbindungen, auch Gewichte genannt, 
können für unterschiedliche Input Daten die entsprechenden Outputs 
gefunden werden.

\begin{figure}[htb]
    \centering
    \label{fig:nn}
    \def\svgwidth{0.5\columnwidth}
    \footnotesize
    \input{Bilder/nn.pdf_tex}
\end{figure}


Die richtige einstellung der Gewichte, welche zunächst zufällig initialisiert werden, 
erfolgt dabei im Trainingsprozess, welcher in \ref{fig:train} schematisch
 dargestellt ist und sich in die drei Schritte:
\begin{itemize}
    \item Feed Forward anhand aktueller Gewichte vorhersage aus den Inputs treffen
    \item Lossfuction Abweichung zu tatsächlichen werten bestimmen
    \item Backpropagation minimierung der Fhlerfunktion durch anpassung der Gewichte
\end{itemize}

\begin{figure}[htb]
    \centering
    \label{fig:train}
    \def\svgwidth{0.5\columnwidth}
    \footnotesize
    \input{Bilder/train_workflow.pdf_tex}
\end{figure}

Durch häufiges wiederholen dieser Schritte kann die Fehlerfunktion soweit minimiert werde, 
dass das Modell auch für neue Input Daten die richtigen Aussagen treffen kann.


%------------------- SUBSECTION: Das Perceptron -------------------

\subsubsection{Vorwärts}\label{subsec:percepron}

Im Vorwärtsdurchgang wird der Input durch alle Schichten hindurch 
gereicht, um in der letzten Schicht den gewünschten Output zu liefern.
Dabei erhält jedes Neuron wie in \ref{fig:neuron} dargestellt, die Ausgaben aller
 neuronen der vorherigen Schicht, summiert diese auf und übergibt den Wert
  einer Aktivierungsfunktion, die den Wert auf einen bestimmten Bereich Skalliert.
 

\begin{figure}[htb]
    \centering
    \input{Bilder/neuron}
    \caption{Einzelnes Perzeptron}
    \label{fig:neuron}
\end{figure}

Die Berechnung des Vorwärtsdurchgangs von einer ges Schicht 
zur nächsten, lässt sich die mithilfe der Matrixixmultiplikation
durchführen, was Gl. in ... als Vektorschreibweise ergibt.
\\
y = a(WTX)\\
wobei a() die Aktivierungsfunktion.
\\
Aktivierungsfunktion können sein
\begin{equation}
    \label{eq:relu}
    \delta(z) = max(0,z)
\end{equation}
ReLU in den hidden Layer
oder

\begin{equation}
    \label{eq:softmax}
    \delta(z) = \frac{e^{z}}{\sum e^{x}}
\end{equation}

\begin{equation}
    \label{eq:sidmoid}
    \delta(z) = \frac{1}{1 + e^{-x}}
\end{equation}


sigmoid(bin) oder softmax(cat) im letzten layer

bei softmax erhällt man Wahrscheinlichkeitsverteilung über allen Output
neuronen.\\
Neben dem Ansatz des Gradienten für die Optimierungs 
gibt es noch weitere, effizientere verfahren wie z.B. Momentum oder Adam.


\subsubsection{Fehlerfunktion}

Die Abweichung der Schätzung, welche an den Neuronen der letzen Schicht 
vorliegen, zu den tatsächlichen Werten, den Labels, wird mithilfe geeigneter 
Fehlerfunktion bestimmt. Für Ragression z.B. abs oder rms und für Kategorisch
häufig logarithmisch.

hier am beispiel einer binären klassifikation (erg 0 oder 1) mit log loss
 (crossentropy) dargestellt.

 \begin{equation}
    \label{eq:crossentropy}
    L = \hat{y}log(y) + (1 - \hat{y})log(1 - y)
\end{equation}

Durch den Logarithmus wird der Loss um so größer, je weiter die Schätzung $y$ vom 
tatsächlichen Wert $\hat{y}$ abweicht.
%hier plot
\subsubsection{Backpropagation}
Durch berechnung des Gradienten der Fehlerfunktion kann ermittelt 
werden in welche Richtung die Gewichte angepasst werden müssen, sodass sie sich 
im nächsten Durchgang minimiert.
Dafür wird die die Fehlerfunktion für jede Schicht partiell nach den 
Gewichten abgeleitet, was wie in gl. \ref{eq:grad} dargestellt mithilfe der 
Kettenregel für die Aktivierungsfunktion geschieht.


\begin{equation}
    \label{eq:grad}
    \frac{\partial L}{\partial w} = \frac{\partial L}{\partial z}\frac{\partial z}{\partial w}
\end{equation}
Damit werden die Gewichte dann nach Gleichung \ref{eq:update_wieghts} angepasst.
\begin{equation}
    \label{eq:update_wieghts}
    w  \leftarrow w - \eta \frac{\partial L}{\partial w}
\end{equation}

wobei die \textit{Leariningrate} $\eta$ die Schrittweite mit der die Anpassung vorgenommen
werden soll angibt.








%------------------- SUBSECTION: Validierung -------------------
\subsection{Validierung und Overfitting}

um überprüfen zu können ob ein Modell die Trainingsdaten tatsächlich generalisiert 
hat, dh auch für neue daten anwendbar ist, oder diese nur auswendig gelernt hat, 
wird häufig der Datensatz in einen Trainingsanteil und einen Testanteil aufgeteilt.

Mit dem Testdatensatz wird dann schon wärend des Trainings regelmäßig zwischen geprüft, 
veringert sich irgendwann nur noch der fehler der trainingsdaten, findet overfitting statt.

% \begin{figure}[htb]
%     \centering
%     \input{Bilder/plot.tex}
%     \caption{irgend ein plot}
%     \label{fig:ptl}
% \end{figure}

Häufig sind zu wenige Trainingsdaten oder zu komplexe/überparametrisierte
 Modelle und damit zuviele freiheitsgrade, grund für overfitting.

% hier plots von linie durch datenpunkte

% Overfitting -> hohe varianz: varianz = train_err - test_error
% Underfitting -> hoher Bias: Bias = 

Techniken um Overfitting zu vermeiden sind z.B.
\begin{itemize}
    \item Augmentierung der Daten
    \item Regularisierung der Parameter (L1/L2)
    \item Dropout
    \item early stopping
\end{itemize}

Bei Augmentierung werden aus den vorhandenen Daten künstlich mehr 
Daten generiert, in dem an den Bildern geometrische transformationen 
oder manipulationen der pixelwerte vorgenommen werden.
\\
Bei Regularisierung wird an die Lossfuction als weiterer Term
 eine aufsummierung der Gewichte gehängt, wodurch diese bei der Minimierung 
  klein gehalten werden, wodurch weniger potential zur überanpassung da ist.
  \begin{equation}
    \label{eq:regularization}
    J(w) = E + \lambda \sum_{i} w_{i}^{2}
\end{equation}

Beim Dropout werden zufällig gewichte zu 0 gesetzt.
\\
early stopping: stoppen des trainings, wenn sich overfitting einstellt.




%------------------- SECTION: DEEP LEARNIN UNG COMPUTER VISION ----------------
\section{Deep Learning und Computer Vision}\label{sec:deepl_cv}

Das maschinelle sehen verwendet Deep Learining Techniken (CNNs) zusammen 
mit Techiniken der Digitalen Bildverarbeitung.
Die für die Bilderkennung am häufigsten eingesetze Art der NNs sind CNNs.

Es geht im wesentlichen darum Bilder und videos zu Klassifizieren oder objekte
in ihenn zu Lokalisieren.


%------------------- SUBSECTION: CNNs -------------------
\subsection{Convolutional Neural Networks}

Um Bilder /Inhalte mithilfe Neuronaler Netze zu erkennen, 
werden die einzelnen Pixelwerte der Bilder als Inputs verwendet 
und das auf dem Bild zu erkennende Objekt als output verwendet. Bilder 
werden als Matrizen der Form $height\times width \times color channels$ 
dargestellt.

Da dies für regulere/vollständig verbundene Neuronale Netze eine 
enorme Anzahl an Parametern und damit einhergenhender rechenkost 
bedeuten würde, werden hier CNNs verwendet, eine Architektur in 
der Paramer von verschiedenen Neuronen gemeinsam genutzt werden.

Hauptbestandteil von CNNs sind die \textit{Convolutional Layers}
welche die mathematische Faltungsoperation zwischen Input Bild 
und Filter/Kernel durchführen.

Die Filter meist der Form $3 \times 3$ oder $5 \times 5$ und mit der 
selben Tiefe wie der Input, werden während des \textit{Forward 
Pass/bropagation} zeilenweise über das Bild geschoben und an jeder Stelle 
das Kreuzprodukt berechnet. Jedes Ergebnis dieser berechnungen ergibt einen
Pixel Wert der nächsten Schicht auch Feature Map genannt. \ref{fig:cnn}

Ein weiterer bestandteil von CNNs sind die Pooling Layer, welche eine 
bestimmte Anzahl an Pixeln zB 3 x 3 zu einem Wert zusammenfassen wodurch 
sich die Parameter Anzahl des Bildes veringert. Das hat den Vorteil, dass 


\begin{figure}[htb]
    \centering
    \label{fig:lenet}
    \includegraphics[width=0.8\textwidth]{Bilder/lenet.png}
    \caption{LeNet-5 cite lecun}
\end{figure}



Ziel dieser Operation ist es, dass die 
Filter Maps bestimmte Muster/features die zu einer bestimmten klasse 
gehren lernen. Mit diesen Filtern können dann unabhängig wo im Bild 
befindlich, die features wie zb horizontale/vertikale linien, Ecken 
oder Kreise gefunden werden. 
\\
Beispiel:
\begin{equation}
    \begin{pmatrix}
        1 & 0 & -1\\
        1 & 0 & -1\\
        1 & 0 & -1
    \end{pmatrix}
\end{equation}

erkennt vertikale Linien im Bild. 




Conv layer:\\
Faltung an cnn erklärt: input image als (h,w,c) tensor wird mit filter/kernel gefaltet. daraus erhält man feature map
zusammen mit pool layer:\\
pool layer erklärt\\

ergibt grundstruktur von cnn\\

weitere layer wie dropout\\

%------------------- SUBSECTION: Transfer Learining ----------------
\subsection{Transfer Learining}

erklären, dass features von einfach bis immer komplexer werdende muster enthalten, die im bild zu finden sind.
\\
filter können zufällig initialisiert und gelernt werden, oder von vortrainierten netzen wieder verwendet
werden. (transfer learining oder fine tuning) da die deatures (besonders in den vorderen layern) immer ähnlich sind und das
neu lernen zeitaufwändig und oft sogar ungenauer ist.
\\
je nach ähnlichkeit des eig datensetz zu dem auf das netz urspr trainiert wurde:\\
scratch, fine tuning, feature extractor


%------------------- SUBSECTION: Competitions ----------------
\subsection{Competitions mit Imagenet und co + cnn winner}\label{subsec:comp}

zuerst competition erklären \\
dann chronologische gewinner netzt + besonderheit\\




%------------------- SUBSECTION: Object Detection ----------------
\subsection{Objekt erkennung}\label{sec:objdet}


Unterschied deutlich machen: klassifikator kann nur ges bild auswertuen und wahscheinlichk angeben welche 
klasse darauf. keine lokalisierng und keine mult obj\\

3 Arten der Bilderkennung: Klassifizierung, Objekt Erkennung (für mult + box), Segmentation (jeden pixel)

dafür obj erkennung notw:\\

Single Shot Detectoren
\\

Two Stage Detectoren


% hier irgendwo die frameworks einbringen
\subsection{Machine Learning Frameworks}

Die Algorithmen müssen nicht jedesmal neu implementiert werden. 
Für die gängigen verfahren gibt es Frameworks, welche 
die Implementierung enthalten und über APIs verwendet 
werden können (Bsp Tensorflo und Keras)


%------------------- SECTION: Hardware ----------------------

\section{Hardware}\label{sec:hardware}
%noch eine section zu Hardware allg (cpu, gpu, tpu), Neural Compute Stick und AI on the egde


allg zu hardware für deeplearning. Das besser auf gpu als cpu. weitere: tpu, fpga, vpu, wie zb ncs2.

\subsection{Neural Compute Stick 2}

technischen spezifikationen




%------------------- SUBSECTION: AI on the Edge ----------------

\subsection{AI on the edge}

was bedeutet dies. cloud unabhängig und ohne groß rechner. bsp anwendungen.

