\chapter{Realisierung Objekterkennung}\label{kap:object_det}

In diesem Kapitel werden zunächst die Beschaffung und Aufbereitung 
der Trainingsdaten beschrieben. Anschließend wird es um das 
Training geeigneter Deep Learning Modelle in Tensorflow sowie 
um die Inferenz dieser in OpenVino gehen.

\section{Datensatz}\label{sec:dataset}

Für das Training eines Deep Learning Modells, werden 
eine Große Menge an Trainingsdaten benötigt.
Handelt es sich um ein Modell zur Objekterkennung,
müssen die Labels neben der Klasse, auch die Koordinaten, 
der Bounding Boxes enthalten.

Die Trainingsdaten können entweder selber erstellt, oder 
aus frei zugänglichen Datensätzen wie z.B. \textit{ImageNet}, 
\textit{COCO}, oder \textit{OpenImages}
aus dem Internet heruntergeladen werden.

Für die Bachelor Arbeit wurden aus dem Open Source Datensatz
\textit{OpenImages} von Google
\cite{kuznetsovaOpenImagesDataset2018}, 
welches 600 gelabelte Klassen enthällt, 
die 9 Klassen \textit{Brown bear, Deer, Fox, Goat, 
Hedgehog, Owl, Rabbit, Raccoon} und \textit{Squirrel}
heruntergeladen und für das Training verwendet.

Für die Evaluierung des Trainings wurde der 
Datensatz, mit dem Verhältnis von 80\%, 10\%, 10\%, in ein
Trainings-, ein Validierungs- und ein 
Testset aufgeteilt.

Je Klasse variierte die Anzahl an Bildern zwischen 200 und 
2000 Stück, wodurch eine Verteilung der Klassen, 
wie in  Abbildung \ref{fig:histo_ohne_aug} dargestelltem Histogramm,
zustande Kam.


\vspace{1cm}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{class_distro.png}
    \captionof{figure}{ohne aug}
    \label{fig:histo_ohne_aug}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{class_distro_aug.png}
    \captionof{figure}{mit aug}
    \label{fig:histo_mit_aug}
\end{minipage}
\vspace{1cm}

Um diese auszugleichen, wurden die Daten, wie im nächsten
Abschnitt genauer beschrieben wird, so augmentiert, 
dass jede Klasse 3000 Bilder vorhanden waren, was zu einer in 
in Abbildung \ref{fig:histo_mit_aug} 
dargestellten Verteilung führte.

Da sich häufig mehrere Tiere der der selben Klasse 
auf einem Bild befinden, weicht, wie in den Histogrammen 
zu erkennen ist, die Anzahl der Bilddateien (3000) von 
der Anzahl der Klassen ab.




\subsection{Augmentierung}\label{subsec:augmentation}


Das Augmentieren von Bilddaten für Deep Learning Modelle,
ist, neben dem ausgleichen der Klasseninbalance, eine sehr
effektive Technik, Overfitting zu verhindern.
Indem geometrische Transformationen oder Manipulationen 
an den Pixelwerten auf die Bilder angewendet werden, 
können diese künstlich vermehrt werden.

Die Augmentierung des OpenImages Datensatzes wurde mithilfe 
eines Pyhton Scripts, in welchem die Library 
\textit{imgaug} \cite{imgaug} verwendet wurde, durchgeführt.
Dabei wurde je, zu augmentierendem Bild, eine geometrische- und 
eine pixelbezogene Transformation, die zufällig 
aus einer Auswahl an Augmentern ausgewählt wurde, angewendet.

In folgendem Codeausschschnitt, des Python Scripts, sind die 
verwendeten Augmentierungstechniken dargestellt.


\begin{lstlisting}[language=Python]
    import imgaug.augmenters as iaa
    
    color_augmenters = [
        iaa.Dropout(p=(0, 0.1)),
        iaa.CoarseDropout((0.01, 0.05), size_percent=0.1),
        iaa.Multiply((0.5, 1.3), per_channel=(0.2)),
        iaa.GaussianBlur(sigma=(0, 5)),
        iaa.AdditiveGaussianNoise(scale=((0, 0.2*255))),
        iaa.ContrastNormalization((0.5, 1.5)),
        iaa.Grayscale(alpha=((0.1, 1))),
        iaa.ElasticTransformation(alpha=(0, 5.0), sigma=0.25),
        iaa.PerspectiveTransform(scale=(0.15)),
        iaa.MultiplyHueAndSaturation((0.7))
    ]

    geometric_augmenters = [
        iaa.Affine(scale=((0.6, 1.2))),
        iaa.Affine(translate_percent=(-0.3, 0.3)),
        iaa.Affine(shear=(-25, 25)),
        iaa.Affine(translate_percent={"x": (-0.3, 0.3), "y": (-0.2, 0.2)}),
        iaa.Fliplr(1),
        iaa.Affine(scale={"x": (0.6, 1.4), "y": (0.6, 1.4)})
    ]
    
\end{lstlisting}

Das Ergebnis, von einigen zufällig angewendeten Augmentierungen 
auf ein Bild der Klasse Fuchs ist in 
Abbildung \ref{fig:augmentierung} dargestellt.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\columnwidth]{Bilder/augmentierung.png}
    \caption{Anwendung von Augmentierungstechniken}
    \label{fig:augmentierung}
\end{figure}


\section{Object detection Modelle}

Object detection Modelle lassen sich, wie in 
\cite{wuRecentAdvancesDeep2019} beschrieben wird,
in Einstufige- und Zweistufige Detektoren einteilen.

Zweistufige Detektoren generieren in der 
ersten Stufe eine Auswahl an räumlichen Vorschlägen
für das Input Bild, in welchen Objekte enthalten sein 
können. 
In der zweiten Stufe, werden die Vorschläge zur Feature 
Extraction einem CNN übergeben, welches neben 
einer Klassifikation auch einen Regressor für 
die Bounding Box Koordinaten besitzt.

Einstufige Verfahren verwenden kein separates 
Netz zur Vorschlagsgenerierung. Stattdessen 
wird das gesamte Bild, gitterartig unterteilt,
als potentielle Regionen für Objekte betrachtet 
und hinsichtlich Vorhandensein einer Klasse je 
Region klassifiziert.
Dadurch sind diese zwar wesentlich schneller, 
aber auch ungenauer.

Für die Bachelor Arbeit wurden 
Modelle beider Ansätze verwendet und 
hinsichtlich Genauigkeit und 
Inferenzzeit miteinander verglichen.

Im Folgenden werden die beiden 
verwendete Modelle näher erläutert.



\subsection*{Faster R-CNN}

Das \textit{Faster R-CNN} \cite{renFasterRCNNRealTime2016a}, 
dargestellt in Abbildung \ref{fig:faster_rcnn}, 
ist ein Modell zur Objekterkennung, welches
ein zweistufiges Verfahren verwendet.

Die Vorschlagsgenerierung erfolgt in einem 
\textit{Region Proposial Networks} (RPN), welches auf 
einem \textit{fully convolutional network} 
basiert, welches Featuer Maps generiert.

Über die Feature Maps werden im 
\textit{Sliding-Window} Verfahren vordefinierte
\textit{Anker Boxen} konvoliert.
Der daraus resultierende Feature Vektor wird 
einem binären klassifikator (\textit{cls layer}), 
welcher angibt ob sich ein Objekt
in dem Vorschlag befindet, 
sowie einem Bounding Regresor
(\textit{reg layer}), zur Lokalisierung,
übergeben.


\vspace{1cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]
    {faster-RCNN_architecture.png}
    \caption{Faster R-CNN Architektur, Quelle:
     \cite{ObjectDetectionDummies2017b}}
     \label{fig:faster_rcnn}
\end{figure}

\subsection*{SSD: Single Shot MultiBox Detector}

Der \textit{Single Shot MultiBox Detector} (SSD)
 \cite{liuSSDSingleShot2016}, 
verwendet ein einstufiges Verfahren zur Objekterkennung,
bei dem das Input Bild gitterartig unterteilt wird.
In jeder Zelle des Gitters werden \textit{default Anker
Boxen} unterschiedlicher skalierungen definiert.

Indem an das Basis CNN weitere \textit{Extra Feature Layer}
verschiedener Größen angehängt werden, kann dieses 
für jede default Box eine Klassifikation, in Form 
eines \textit{confidence scores}, sowie eine Lokalisierung, 
in Form eines Offsets zur default Box, vornehmen.

Diese werden zur finalen Detektion 
einem \textit{non-maximum suppression}
Layer \cite{Hosang2017cvpr} übergeben,
welcher alle, zu einer Klasse gehörenden Boxen,
zu einer Box zusammenführt.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]
    {ssd_architecture.png}
    \caption{SSD, quelle: \cite{SSDSingleShot}}
    \label{fig:faster_rcnn}
\end{figure}



\section{Training}

Das Training der Deep Learning Modelle erfolgte in dem 
Framework Tensorflow, welches auch von 
OpenVino, für den Neural Compute Stick, uterstütz wird.
Dabei wurde eine speziell für die Object detection 
entwickelte Api von Tensorflow verwendet.

Um unabhängig von der Leistungsfähigkeit der GPU des 
Rechners zu sein, wurde das Training in der Cloudbasierten
Virtual Machine \textit{Google Colab} \cite{colab} durchgeführt,
welche kostenlos eine, für Deep Learning geeignete, GPU
zur verfügung stellt.



\subsection{Tensorflow Object Detection Api}

Die \textit{Tensorflow Object Detection Api} ist unter den
Research Modellen des offiziellen Tensorflow
Repositorys auf GitHub zu finden \cite{tfobjdet},
und enthällt Implementierungen
einiger gängiger Object detection Modelle, mit verschiedenen 
vortrainerten Basis CNNs.

Der, für die Bachelor Arbeit Verwendete, \textit{Single 
Shot Detector} (SSD), wurde zum Einen mit dem 
\textit{MobilenetV2} und zum Anderen mit dem 
\textit{InceptionV2}, als Basis CNN trainiert.
Für das \textit{Faster R-CNN} wurde, aufgrund 
der Verfügbarkeit, nur mit dem \textit{InceptionV2}
trainiert.

Um die Modelle trainieren zu können, mussten zunächst die 
Trainingsdaten in das binäre Dateiformat \textit{TFRecords} 
umgewandelt werden, welches die Tensorflow Api verwendet.
Dieses ist eine serialisierte 
Darstellung der Bilder und Labelfiles als Protocol Buffer,
welche einen schnelleren Zugriff auf die Daten ermöglichen.

Parameter für das Modell konnten vor dem Training 
in einer Konfigurationsdatei festgelegt werden.

Diese wurde dann, zusammen mit den TFRecord Files, 
dem Konsolen-Kommando, mit dem das Training 
gestartet wurde, übergeben.

Während des Trainings wurden, in regelmäßigen 
Abständen, die trainierten Gewichte abgespeichert.

Mithilfe das Evaluierungstools \textit{Tensorboard}
konnte der Trainingsfortschrit anhand bestimmter
Metriken angezeigt und ausgewertet werden.

So konnten schon während des Trainings fehlerhafte
Einstellungen der Datensatz- und 
Modelkonfiguration festgestellt 
und korrigiert werde, indem z.B. andere
Augmentierungstechniken verwendet,
oder Hyperparameter des Models umgestellt wurden.

In Abbildung \ref{fig:train_workflow}
ist der Trainingsworkflow dargestellt 
welcher dabei zustande kam.

Die Ergebnisse der Trainierten Modelle 
werden im nächsten Kapitel diskutiert.


\vspace{1cm}
\begin{figure}[H]
    \centering
    \input{Bilder/train_workflow_gesammt.tex}
    \caption{Trainingsworkflow}
    \label{fig:train_workflow}
\end{figure}
\vspace{1cm}


\section{Inferenz}\label{sec:inferenz}

Die Anwendung, eines fertig trainierten Modells, für 
neue Inputdaten, wird als \textit{Inferenz} bezeichnet.
Zur Ausführung dieser, auf dem Neural Compute Stick 2, 
wird das Toolkit \textit{OpenVino} von Intel verwendet.
Dafür musste zunächst der trainierte Tensorflow Graph 
exportiert, d.h. die aktuellen Werte der Gewichte 
eingefroren werden.

Anschließend konnte mit dem \textit{Model Optimizer}
das Modell in die Intermediate Representation (IR)
konvertiert werden.
Diese besteht aus einer xml und einer bin Datei und 
und kann von der \textit{Inferecne Egine} 
zur Inferenz gelesen werden.


\subsection*{InferecneEgine}

Um die Inferenz eines Modells im IR-Format 
auf dem NCS2 ausführen zu können, werden, 
in der Inference Engine, die in Abbildung 
\ref{fig:inger_engine_workflow} schematisch
dargestellten Schritte, durchgeführt.
Daneben ist jeweils die entsprechende 
Codezeile in Python dargestellt.

Zunächst wird das Zielgerät, auf dem 
die Inferenz ausgeführt werden soll,
spezifiziert (\textit{HW Plugin laden}).

Anschließend wird das Modell anhand der 
IR Dateien definiert (\textit{Model IR einlesen})
woraus sich die In- und Outputblobs generieren 
lassen, (\textit{In-und Outputblob}), 
welche die Diemensionen der Ein- und Ausgabe 
Schicht des Modells darstellen.

Das zu inferierende Bild,
welches als Matrix aus Pixelwerten 
vorliegt, muss dann in das \textit{Input Blob}
Format gebracht werden (\textit{process Input}).

Nachdem das Bild inferiert wurde (\textit{Inferenz}),
kann es, zusammen mit den Inferenzergebnissen, 
weiterverarbeitet werden (\textit{process
output}).

Handelt es sich bei den Inputs 
um einen fortlaufenden Video- oder 
Kamera Stream, werden die Schritte 
\textit{preprocess}, \textit{Inferenz} und 
\textit{process Output} in einer Schleife wiederholt.


\vspace{1cm}
\begin{minipage}{0.30\textwidth}
    \centering
    \input{Bilder/inferece_engine_diagram}
    \captionof{figure}{}
    \label{fig:inger_engine_workflow}
\end{minipage}
\begin{minipage}{0.70\textwidth}

%\begin{python}
\begin{lstlisting}[language=Python]

    plugin = IEPlugin(device='MYRIAD')

        
    net = IENetwork(model_xml, model_bin)
        
    
    input_blob  = net.inputs
    output_blob = net.outputs
        

    exec_net = plugin.load_network(net, n_req)
        
    while True:

        image = preprocess(capture) # hwc -> nchw
        
        
        res = exec_net.infer({input_blob : image})
        

        res = res[output_blob]
        
        
\end{lstlisting}
%\end{python}
\vspace{1.5cm}
\end{minipage}
\vspace{1cm}

% transform:
% # capture dims zu input blob dims transformieren
% # img_h, img_w, img_c -> blob_n, blob_c, blob_h, blob_w

Die Form des Inferenzergebnisses hängt von der 
Art des verwendeten Deep Learning Modells ab, welches 
z.B. \textit{Image Classification}, \textit{Object detection},
oder \textit{Instance
Segmentation} sein können.

Für Object detection Modelle enthällt das Ergebnis
 Datenstrukturen, welche den Index, 
 zugehörige Wahrscheinlichkeit, sowie Bounding 
 Box Koordinaten geschätzen Objekte im Bild enthalten.

Indem man für alle Schätzungen, die in einem Bild gemacht wurden, 
einen Threshhold für die Wahrscheinlichkeit festlegt (zb. 70\%),
könenn die sinnvollen Ergebnisse herausgefiltert werdsen.

Zur veranschaulichung können die Koordinaten dann 
als Bounding Box in das inferierte Bild gezeichnet werden.


