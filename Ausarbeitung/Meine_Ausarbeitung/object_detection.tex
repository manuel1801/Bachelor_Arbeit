\chapter{Realisierung Objekt Erkennung}\label{kap:object_det}

\section{Datensatz}\label{sec:dataset}

Um ein Deep Learning Modell richtig trainieren zu können, 
wird eine große Menge an gelabelten Trainingsdaten benötigt.
Handelt es sich dabei um ein, wie in Abschnitt
\ref{sec:related_work} beschriebenen  Modell zur Objekterkennung
müssen die Labels neben der Klasse, auch die Koordinaten, 
die angeben wo sich das Objekt im Bild befindet. 
Indem die, sogenannten Bouning Boxes, mit traininert werden, 
kann das Modell auch die Lokalisierung der Objekte vorhersagen.
Die Trainingsdaten können entweder selber erstellt, oder 
aus frei zugänglichen Datensätzen wie z.B. \textit{ImageNet}, 
\textit{COCO}, oder \textit{OpenImages}
aus dem Internet heruntergeladen werden.

Für die Arbeit wurden aus dem Open Source Dataset
\textit{OpenImages} von Google
\cite{kuznetsovaOpenImagesDataset2018}, 
welches 600 gelabelte Klassen enthällt, 
die 9 Klassen \textit{Brown bear, Deer, Fox, Goat, 
Hedgehog, Owl, Rabbit, Raccoon} und \textit{Squirrel}
heruntergeladen und für das Training verwendet.
Für die Evaluierung des Trainings wurde der 
Datensatz in ein Trainings, einen Validierung und einen 
Testset aufgeteilt, mit dem Verhälltnis 80\%, 10\%, 10\%.
Je Klasse variierte die Anzahl an Bildern zwischen 200 und 
2000, wudurch die in Abbildung \ref{fig:histo_ohne_aug}
dargestellte unausgeglichenheit der Klassen zustande kam.
Um diese auszugleichen, wurden die Date, wie im nächsten Abschnitt 
genauer erklärt wird, so augmentiert, das je Klasse 3000 Bilder 
vorhanden waren, was zu einer in Abbildung \ref{fig:histo_mit_aug}
dargestellten verteilung der Klassen führte.

\vspace{1cm}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{class_distro.png}
    \captionof{figure}{ohne aug}
    \label{fig:histo_ohne_aug}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{class_distro_aug.png}
    \captionof{figure}{mit aug}
    \label{fig:histo_mit_aug}
\end{minipage}
\vspace{1cm}

Der Grund dafür, dass in den Histogrammen mehr Klassen,
als es tatsächlich Bilder 
waren angezeigt werden, kommt daher, dass 
sich häufig mehrere Tiere der selben Klasse in einem Bild 
befinden.
Ein weiterer Grund für die Augmentierung der Daten war 
die vorsogliche Maßnahme gegen Overfitting.




\subsection{Augmentierung}\label{subsec:augmentation}


Das augmentieren von Bilddaten für Deep Learning, 
ist eine effektive Technik, durch künstliches generieren
neuer Daten aus den vorhandenen, um Overfitting zu
verhindern und Inbalancen der Klassen untereinander auszugleichen.
Die vermehrung der Daten erfolg dabei in dem die Bilder 
geometrischen Transformationen oder manipulationen der Pixelwerte 
unterzogen werden.

Die Augmentierung des OpenImages Datensatzes wurde mithilfe 
eines Pyhton Scripts, welches die Library \textit{imgaug} \cite{imgaug}
verwendet, durchgeführt.
Dabei wurde je zu augmentierendem Bild eine geometrische und 
eine Pixel Transformation angewendet.
Diese wurden zufällig aus einer Auswahl an Augmentern, 
angewendetet.

Folgend ist ein Codeausschschnitt des Python Scripts dargestellt, 
welcher die verwendeten Augmentierungstechniken zeigt.


\begin{lstlisting}[language=Python]
    import imgaug.augmenters as iaa

    color_augmenters = [
        iaa.Dropout(p=(0, 0.1)),
        iaa.CoarseDropout((0.01, 0.05), size_percent=0.1),
        iaa.Multiply((0.5, 1.3), per_channel=(0.2)),
        iaa.GaussianBlur(sigma=(0, 5)),
        iaa.AdditiveGaussianNoise(scale=((0, 0.2*255))),
        iaa.ContrastNormalization((0.5, 1.5)),
        iaa.Grayscale(alpha=((0.1, 1))),
        iaa.ElasticTransformation(alpha=(0, 5.0), sigma=0.25),
        iaa.PerspectiveTransform(scale=(0.15)),
        iaa.MultiplyHueAndSaturation((0.7))
    ]

    geometric_augmenters = [
        iaa.Affine(scale=((0.6, 1.2))),
        iaa.Affine(translate_percent=(-0.3, 0.3)),
        iaa.Affine(shear=(-25, 25)),
        iaa.Affine(translate_percent={"x": (-0.3, 0.3), "y": (-0.2, 0.2)}),
        iaa.Fliplr(1),
        iaa.Affine(scale={"x": (0.6, 1.4), "y": (0.6, 1.4)})
    ]
    
\end{lstlisting}

In Abbildung \ref{fig:augmentierung} sind beispielhaft 9 dieser 
zufällig augmentierten Bilder der Klasse Fuchs dargestellt.


\begin{figure}[H]
    \label{fig:augmentierung}
    \centering
    \includegraphics[width=0.95\columnwidth]{Bilder/augmentierung.png}
    \caption{Anwendung von Augmentierungstechniken}
\end{figure}


\section{Object Detection Modelle}


Object Detectione Modelle lassen sich, wie in 
\cite{wuRecentAdvancesDeep2019} beschrieben wird,
in Einstufige- und Zweistufige Detektoren einteilen.

Zweistufige Detektoren generieren in der 
ersten Stufe, 
eine Auswahl an räumlichen Vorschlägen, in welchen 
Objekte enthalten sein können. In der zweiten Stufe, 
werden die Vorschläge zur Feature Extraction 
einem CNN übergeben, welchem ein Netz zur Klassifizierung 
(zb SVM) und eines zur annäherung der Bounding Box 
Koordinaten (Regressor) folgen.

Einstufige Verfahren verwenden kein seperates 
Netz zur Vorschlagsgenerierung. Stattdessen 
wird das gesamte Bild, gitterartig unterteilt,
als potentielle Regionen für Objekte betrachtet 
und hisichtlich vorhandensein einer Klasse je 
Region klassifiziert.
Dadurch sind diese zwar wesentlich schneller, 
aber auch ungenauer.


Für die Thesis wurden beide Ansätze verwendet und 
für die Umsetzbarkeit als Wildtierdetektor 
mit dem NCS2 hinsichtlich Genauigkeit und 
Inferenzzeit verglichen.

Die dafür verwendeten Modelle werden im folgenden 
näher erläutert.



\subsection*{Faster R-CNN}

Das Faster R-CNN \cite{renFasterRCNNRealTime2016a}, 
dargestellt in Abbildung \ref{fig:faster_rcnn}, 
verwendet ein zweistufiges Verfahren zu 
Objekterkennung.

Die Vorschlagsgenerierung erfolgt mithilfe eines 
Region Proposial Networks (RPN), welches auf 
einem \textit{fully convolutional network} 
basiert.
Über die daraus generierten Feature Maps werden im 
Sliding-Window Verfahren vordefinierte Anker Boxen 
konvoliert.
Der daraus resultieruende Feature Vektor wird 
einem binären klassifikator (\textit{cls layer}), 
welcher angibt ob sich ein Objekt in dem Vorschlag befindet, 
sowie einem Bounding Regresor (\textit{reg layer}) übergeben.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \label{fig:faster_rcnn}
    \includegraphics[width=0.95\textwidth]
    {faster-RCNN_architecture.png}
    \caption{Faster R-CNN Architektur, Quelle:
     \cite{ObjectDetectionDummies2017b}}
\end{figure}




\subsection*{SSD: Single Shot MultiBox Detector}

Der Single Shot MultiBox Detector (SSD) \cite{liuSSDSingleShot2016} 
ist ein einstufiges Verfahren zur Objekterkennung, 
welches das Input Bild gitterartig unterteilt. In jeder Zelle des 
Gitters werden default Anker Boxen unterschiedlicher 
skalierungen definiert.

Indem an das Backbone CNN weitere \textit{Extra Feature Layer}
verschiedener größe angehängt werden, kann dieses 
für jede default Box eine klassifizierungen, in form 
eines \textit{confidence scores} sowie eine Lokalisierung, 
in Form eines Offsets zur default Box vornehmen.

Diese werden zur finalen Detektion einem \textit{non-maximum suppression}
Layer \cite{Hosang2017cvpr} übergeben,
welcher alle zu einer klasse gehörenden boxen zusammenführt.



\begin{figure}[H]
    \centering
    \label{fig:faster_rcnn}
    \includegraphics[width=0.95\textwidth]{ssd_architecture.png}
    \caption{SSD, quelle: \cite{SSDSingleShot}}
\end{figure}




\section{Training}

Das Training der Deep Learning Modelle erfolgte mithilfe 
des Frameworks Tensorflow, welches auch von 
Open Vino für den Neural Compute Stick uterstütz wird.
Dabei wurde eine speziell für Object Detection 
entwickelte Api von Tensorflow verwendet.

Um unabhängig von der leistungsfähigkeit der GPU des eigenen 
Rechners zu sein, wurde das Training in der Cloudbasierten Virtual 
Machine \textit{Google Colab} \cite{colab} durchgeführt, welche kostenlos eine für 
Deep Learning geeignete GPU zur verfügung stellt.



\subsection{Tensorflow Object Detection Api}



Die Tensorflow Object Detection Api ist unter den Research Modellen
\cite{tfobjdet} des offiziellen Tensorflow Repository auf GitHub zu
finden und enthällt implementierungen einiger gängiger Object Detectin
Modelle mit vortrainierten gewichten zur Feature Extraction.
Für die Arbeit wurde die im Abschnitt \ref{sec:related_work} 
erläuterten Architekturen \textit{Single Shot Detector (SSD)}
und \textit{Faster R-CNN} verwendet.
Beim SSD wurde das \textit{MobilenetV2} sowie das \textit{InceptionV2} 
als Basis CNN verwendet.
Bei Faster R-CNN nur \textit{InceptionV2}, da hierfür das MobilenetV2 
nicht verfügbar war.
Weitere Basis CNN sind unter Anderem das Resudial Network, welche jedoch 
sehr Rechenaufwendig und nicht für den Neural Compute Stick 
kompatibel ist.

Die Festlegung der Parameter für die Modelle kann dabei 
in einer Konfigurationsdatei vorgenommen werde.


Um die Modelle trainieren zu können, mussten zunächst die 
Trainingsdaten in das binary Dateiformate TFRecords umgewandelt 
werden, welches die Api verwendet. Dieses ist eine Serialisierte 
darstellung der Bilder und Labels als Protocol Buffer welche einen 
schnelleren Zugriff auf die Daten ermöglicht.

Nach herunterladen des Modellund festlegen einiger Parameter was in 
einem Jupyter Notebook der Colab VM erfolgte, konnte das Training 
gestatret werden. 

Das erfolgte über einen Command dem die TfRecord Files als Argument 
übergebun wurden. 
Die Trainierten Gewichte wurden während des Trainings regelmäßig abgespeichert.

Mithilfe das Evaluierungstools Tensorboard konnte der Trainingsfortschrit 
mitverfolgt und ausgewertet werden.


\subsection{Trainingsworkflow}

Durch die in Tensorboard visualisierte beobachtung der 
Trainingsverläufe, konnten so schon während des traings fehlerhafte 
Einstellung der Trainingskonfigurarion festgestellt und korrigiert 
werden.

So ergibt sich der in ... dargestellte Workflow, in dem 
durch unterschiedliche Konfigurationen der für das Training 
benötigten Schritte, das Optimalste Endergebniss erzielt 
werden kann.


Anpasungen die für die trainierten Modelle SSD und Faster 
R-CNN vorgenommen wurden bezogen sich auf unterschiedliche 
Augmentierung der Trainingsdaten, sowie verschiedene 
einstellungen der Hyperparameter.


% \begin{figure}[H]
%     \centering
%     \def\svgwidth{0.7\textwidth}
%     \input{Bilder/train_workflow_gesammt.pdf_tex}
%     \caption{Trainingsworkflow}
%     \label{fig:train_workflow}
% \end{figure}

\vspace{1cm}
\begin{figure}[H]
    \centering
    \input{Bilder/train_workflow_gesammt.tex}
    \caption{Trainingsworkflow}
    \label{fig:train_workflow}
\end{figure}
\vspace{1cm}
Die Ergebnisse der jeweiligen Ansätze werden im nächsten Kapitel diskutiert.


\section{Inferenz}\label{sec:inferenz}

Die Anwendung eines fertig trainierten Modells für 
neue Inputdaten, wird als Inferenz bezeichnet.
Zur Ausführung dieser auf dem NCS2, wird wie in 
den Grundlagen beschriebene das Toolkit \textit{OpenVino}
von Intel verwendet.

Dafür musste zunächst der trainierte Tensorflow Graph 
exportiert, d.h. die aktuellen Werte der Gewichte 
konstant gesetzt werden.

Mit dem ModelOptimizer, der ebnfalls im OpenVino 
Toolkit enthalten war konnte das Tensorflow Modell 
in die Intermediate Representation (IR), bestehend 
aus einer xml und einer bin Datei konvertiert werden.


Diese können nun von der InferecneEgine zur Inferenz weiter 
verwendet werden.


\subsection*{InferecneEgine}


Die für die Inferenz eines Modells im IR Formal 
auf dem NCS2 notwendigen Schritte, welche die
 InferecneEgine ausführt, sind in Abbildugn
\ref{fig:inger_engine_workflow} schematisch dargestellt.

Daneben ist jeweils (teils vereinfacht) die entsprechende 
Codezeile in Python abgebildet.

Zunächst muss das Zielgerät spezifiziert (\textit{HW Plugin laden})
und das Model anhand der IR Dateien definiert werden
(\textit{Model IR einlesen}), um daraus 
das Ausführbare Model erzugen zu können (\textit{executable Model}).

\textit{In-und Outputblob} sind die diemensionen welche
das Model in den In-und Output Schichten hat
und an die das Array des zu inferierenden 
Bilds angepasst werden müssen, was im Schritt 
\textit{preprocess Input} geschieht.

Nachdem die Inferenz abschossen ist, können die 
ergebnisse weter verarbeitet werden, handelt es sich 
die Inferenz eines Videos oder Kamera Streams, 
werden die Schritte preprocess, Inferenz und 
process Output in einer Schleife wiederholt.

\vspace{1cm}
\begin{minipage}{0.30\textwidth}
    \centering
    \input{Bilder/inferece_engine_diagram}
    \captionof{figure}{}
    \label{fig:inger_engine_workflow}
\end{minipage}
\begin{minipage}{0.70\textwidth}

%\begin{python}
\begin{lstlisting}[language=Python]

    plugin = IEPlugin(device='MYRIAD')

        
    net = IENetwork(model_xml, model_bin)
        
    
    input_blob  = net.inputs
    output_blob = net.outputs
        

    exec_net = plugin.load_network(net, n_req)
        
    while True:

        image = preprocess(capture) # hwc -> nchw
        
        
        res = exec_net.infer({input_blob : image})
        

        res = res[output_blob] #
        
        
\end{lstlisting}
%\end{python}
\vspace{1.5cm}
\end{minipage}
\vspace{1cm}


% transform:
% # capture dims zu input blob dims transformieren
% # img_h, img_w, img_c -> blob_n, blob_c, blob_h, blob_w

Die Form des Inferenz Ergebnisses hängt von der 
Art des verwendeten Deep Learning Modells ab, welche 
z.B. Image Classification, ObjectDetection, oder Instance
Segmentation sein können.

Für Object Detection Modelle enthällt das Ergebnis Datenstrukturen, 
welche den Index der Klassen, zugehörige Wahrscheinlichkeit, 
sowie Boxkoordinate der geschätzen Objekte im Bild enthalten.

Indem man für alle Schätzungen die in einem Bild gemacht werden 
einen Threshhold für die Wahrscheinlichkeit festlegt (zb. 0,7),
könenn die sinnvollen Ergebnisse herausgefiltert werdsen.

Zur veranschaulichung können die Koordinaten dann 
als Bounding Box in das Inferierte Bild gezeichnet werden.
