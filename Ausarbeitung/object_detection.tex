\chapter{Realisierung Objekterkennung}\label{kap:object_det}

In diesem Kapitel werden zunächst die Beschaffung und Aufbereitung 
der Trainingsdaten beschrieben. Anschließend wird es um das 
Training geeigneter Deep Learning Modelle in \textit{Tensorflow}
 sowie um die Inferenz dieser in \textit{OpenVino} gehen.

\section{Datensatz}\label{sec:dataset}

Für das Training eines Deep Learning Modells werden 
eine Große Menge an Trainingsdaten benötigt.
Handelt es sich um ein Modell zur Objekterkennung,
müssen die Labels neben der Klasse, auch die Koordinaten, 
der \textit{Bounding Boxen} enthalten.

Die Trainingsdaten können entweder selber erstellt, oder 
aus frei zugänglichen Datensätzen wie z.B. \textit{ImageNet}, 
\textit{COCO}, oder \textit{OpenImages}
aus dem Internet heruntergeladen werden.

Für die Bachelor Arbeit wurden aus dem Open Source Datensatz
\textit{OpenImages} von Google
\cite{kuznetsovaOpenImagesDataset2018}, 
welches 600 gelabelte Klassen enthällt, 
die 9 Klassen \textit{Brown bear, Deer, Fox, Goat, 
Hedgehog, Owl, Rabbit, Raccoon} und \textit{Squirrel}
heruntergeladen und für das Training verwendet.

Für die Evaluierung des Trainings wurde der 
Datensatz, mit einem Verhältnis von 80\%, 10\%, 10\%, in ein
Trainings-, ein Validierungs- und ein 
Testset aufgeteilt.

Je Klasse variierte die Anzahl an Bildern zwischen 200 und 
2000 Stück, wodurch eine Verteilung der Klassen, 
wie in  Abbildung \ref{fig:histo_ohne_aug} dargestelltem
Histogramm, zustande Kam.

\vspace{1cm}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{class_distro.png}
    \captionof{figure}{Ohne Augmentierung}
    \label{fig:histo_ohne_aug}
\end{minipage}
\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{class_distro_aug.png}
    \captionof{figure}{Ohne Augmentierung}
    \label{fig:histo_mit_aug}
\end{minipage}
\vspace{1cm}

Um diese auszugleichen, wurden die Daten, wie im nächsten
Abschnitt genauer beschrieben wird, so augmentiert, 
dass für jede Klasse 3000 Bilder vorhanden waren, was zu einer in 
in Abbildung \ref{fig:histo_mit_aug} 
dargestellten Verteilung führte.

Da sich häufig mehrere Tiere der selben Klasse 
auf einem Bild befinden, weicht, wie in den Histogrammen 
zu erkennen ist, die Anzahl der Bilddateien (3000) von 
der Anzahl der Klassen ab.



\subsection{Augmentierung}\label{subsec:augmentation}

Das Augmentieren von Bilddaten für Deep Learning Modelle
ist, neben dem Ausgleichen der Klasseninbalance, eine sehr
effektive Technik, Overfitting zu verhindern.
Indem geometrische Transformationen oder Manipulationen 
an den Pixelwerten auf die Bilder angewendet werden, 
können diese künstlich vermehrt werden.

Die Augmentierung des \textit{OpenImages} Datensatzes wurde mithilfe 
eines Pyhton Scripts, in welchem die Library 
\textit{imgaug} \cite{imgaug} verwendet wurde, durchgeführt.
Dabei wurde, je zu augmentierendem Bild, eine geometrische- und 
eine pixelbezogene Transformation angewendet, die dabei 
zufällig aus einer Auswahl an Augmentern ausgewählt wurde.

In folgendem Codeausschschnitt, des Python Scripts, sind die 
verwendeten Augmentierungstechniken dargestellt.

\begin{lstlisting}[language=Python]
    import imgaug.augmenters as iaa
    
    color_augmenters = [
        iaa.Dropout(p=(0, 0.1)),
        iaa.CoarseDropout((0.01, 0.05), size_percent=0.1),
        iaa.Multiply((0.5, 1.3), per_channel=(0.2)),
        iaa.GaussianBlur(sigma=(0, 5)),
        iaa.AdditiveGaussianNoise(scale=((0, 0.2*255))),
        iaa.ContrastNormalization((0.5, 1.5)),
        iaa.Grayscale(alpha=((0.1, 1))),
        iaa.ElasticTransformation(alpha=(0, 5.0), sigma=0.25),
        iaa.PerspectiveTransform(scale=(0.15)),
        iaa.MultiplyHueAndSaturation((0.7))
    ]

    geometric_augmenters = [
        iaa.Affine(scale=((0.6, 1.2))),
        iaa.Affine(translate_percent=(-0.3, 0.3)),
        iaa.Affine(shear=(-25, 25)),
        iaa.Affine(translate_percent={"x": (-0.3, 0.3), "y": (-0.2, 0.2)}),
        iaa.Fliplr(1),
        iaa.Affine(scale={"x": (0.6, 1.4), "y": (0.6, 1.4)})
    ]
    
\end{lstlisting}

Das Ergebnis von einigen zufällig angewendeten Augmentierungen 
auf ein Bild der Klasse Fuchs, ist
in Abbildung \ref{fig:augmentierung} dargestellt.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\columnwidth]{Bilder/augmentierung.png}
    \caption{Anwendung von Augmentierungstechniken}
    \label{fig:augmentierung}
\end{figure}


\section{Object detection Modelle}

Object detection Modelle lassen sich, wie in 
\cite{wuRecentAdvancesDeep2019} beschrieben wird,
in Einstufige- und Zweistufige Detektoren einteilen.

Zweistufige Detektoren generieren in der 
ersten Stufe eine Auswahl an räumlichen Vorschlägen
für das Inputbild, in welchen Objekte enthalten sein 
können. 
In der zweiten Stufe, werden die Vorschläge zur \textit{Feature 
Extraction} einem \Gls{cnn} übergeben, welches neben 
einem Klassifikator auch einen \textit{Regressor} für 
die \textit{Bounding Box} Koordinaten besitzt.

Einstufige Verfahren verwenden kein separates 
Netz zur Vorschlagsgenerierung. Stattdessen 
wird das gesamte Bild als potentielle
Region für Objekte betrachtet, indem 
dieses Gitterartig unterteilt wird.
Jeder Teil, der eine mögliche Region darstellt, 
wird dann hinsichtlich Vorhandensein 
eines Objekt, klassifiziert.

Für die Bachelor Arbeit wurden 
Modelle beider Ansätze verwendet und 
hinsichtlich Genauigkeit und 
Inferenzzeit miteinander verglichen.

Im Folgenden werden die beiden 
verwendete Modelle näher erläutert.


\subsection*{Faster R-CNN}

Das \textit{Faster R-CNN} \cite{renFasterRCNNRealTime2016a}, 
dargestellt in Abbildung \ref{fig:faster_rcnn}, 
ist ein Modell zur Objekterkennung, welches
ein zweistufiges Verfahren verwendet.

Die Vorschlagsgenerierung erfolgt in einem 
\textit{Region Proposial Networks} (RPN), welches auf 
einem \textit{fully convolutional network} 
basiert, welches \textit{\Glspl{featuremap}} generiert.

Über die \textit{\Glspl{featuremap}} werden im 
\textit{Sliding-Window} Verfahren vordefinierte
\textit{Anker Boxen} konvoliert.
Der daraus resultierende Feature Vektor wird 
einem binären Klassifikator (\textit{cls layer}), 
welcher angibt ob sich ein Objekt
in dem Vorschlag befindet, 
sowie einem Bounding Box Regresor
(\textit{reg layer}) zur Lokalisierung,
übergeben.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]
    {faster-RCNN_architecture.png}
    \caption{Faster R-CNN Architektur,
     \cite{ObjectDetectionDummies2017b}}
     \label{fig:faster_rcnn}
\end{figure}

\subsection*{SSD: Single Shot MultiBox Detector}

Der \textit{Single Shot MultiBox Detector} (SSD)
 \cite{liuSSDSingleShot2016}, 
verwendet ein einstufiges Verfahren zur Objekterkennung,
bei dem das Input Bild gitterartig unterteilt wird.
In jeder Zelle des Gitters werden \textit{default Anker
Boxen} unterschiedlicher skalierungen definiert.

Indem an das Basis CNN weitere \textit{Extra Feature Layer}
verschiedener Größen angehängt werden, kann dieses, 
für jede default-Box, eine Klassifikation, in Form 
eines \textit{confidence scores}, sowie eine Lokalisierung, 
in Form eines \textit{Offsets} zur default-Box, vornehmen.

Diese werden zur finalen Detektion 
einem \textit{non-maximum suppression}
Layer \cite{Hosang2017cvpr} übergeben,
welcher alle, zu einer Klasse gehörenden Boxen,
in einer Box zusammenführt.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]
    {ssd_architecture.png}
    \caption{SSD Architektur, \cite{SSDSingleShot}}
    \label{fig:faster_rcnn}
\end{figure}



\section{Training}

Das Training der Deep Learning Modelle erfolgte in dem 
\Gls{framework} \textit{Tensorflow}, welches auch von 
\textit{OpenVino}, für den \textit{Neural Compute Stick},
unterstützt wird.
Dabei wurde eine speziell für die Object detection 
entwickelte \Gls{api} von \textit{Tensorflow} verwendet.

Um unabhängig von der Leistungsfähigkeit der \Gls{gpu} des 
Rechners zu sein, wurde das Training in der Cloudbasierten
\Gls{vm} \textit{Google Colab} \cite{colab} durchgeführt,
welche kostenlos eine, für Deep Learning geeignete, \Gls{gpu}
zur Verfügung stellt.



\subsection{Tensorflow Object Detection Api}

Die \textit{Tensorflow Object Detection Api} ist unter den
\textit{Research Modellen} des offiziellen Tensorflow
Repositorys auf \textit{GitHub} zu finden \cite{tfobjdet},
und enthält Implementierungen
einiger gängiger Object detection Modelle, mit verschiedenen 
vortrainerten Basis-\Glspl{cnn}.

Der, für die Bachelor Arbeit Verwendete, \textit{Single 
Shot Detector} (SSD), wurde zum einen mit dem 
\textit{MobilenetV2} und zum anderen mit dem 
\textit{InceptionV2} als Basis CNN trainiert.
Für das \textit{Faster R-CNN} wurde, aufgrund 
der Verfügbarkeit, nur mit dem \textit{InceptionV2}
trainiert.

Um die Modelle trainieren zu können, mussten zunächst die 
Trainingsdaten in das binäre Dateiformat \textit{TFRecords} 
umgewandelt werden, welches die Tensorflow Api verwendet.
Dieses ist eine serialisierte 
Darstellung der Bilder und Labelfiles als \textit{Protocol Buffer},
welche einen schnelleren Zugriff auf die Daten ermöglichen.

Parameter für das Modell konnten vor dem Training 
in einer Konfigurationsdatei festgelegt werden.

Diese wurde dann, zusammen mit den \textit{TFRecord} Dateien, 
dem Konsolen-Kommando, mit dem das Training 
gestartet wurde, übergeben.

Während des Trainings wurden in regelmäßigen 
Abständen die trainierten Gewichte abgespeichert.

Mithilfe das Evaluierungstools \textit{Tensorboard}
konnte der Trainingsfortschritt, anhand bestimmter
Metriken, angezeigt und ausgewertet werden.

So konnten schon während des Trainings fehlerhafte
Einstellungen der Datensatz- und 
Modelkonfiguration festgestellt 
und korrigiert werde, indem z.B. andere
Augmentierungstechniken verwendet,
oder \textit{Hyperparameter} des
Models umgestellt wurden.

In Abbildung \ref{fig:train_workflow}
ist der Trainingsworkflow dargestellt 
welcher dabei zustande kam.

Die Ergebnisse der trainierten Modelle 
werden im nächsten Kapitel diskutiert.
\vspace{1cm}

\begin{figure}[H]
    \centering
    \input{Bilder/train_workflow_gesammt.tex}
    \caption{Trainingsworkflow}
    \label{fig:train_workflow}
\end{figure}
\vspace{1cm}


\section{Inferenz}\label{sec:inferenz}

Die Anwendung, eines fertig trainierten Modells, für 
neue Inputdaten, wird als \textit{Inferenz} bezeichnet.
Zur Ausführung dieser, auf dem \textit{Neural Compute Stick 2}, 
wird das Toolkit \textit{OpenVino} von \textit{Intel} verwendet.
Dafür musste zunächst der trainierte \textit{Tensorflow Graph} 
exportiert, d.h. die aktuellen Werte der Gewichte 
eingefroren werden.

Anschließend konnte mit dem \textit{Model Optimizer}
das Modell in die \textit{Intermediate Representation} (IR)
konvertiert werden.
Diese besteht aus einer .xml- und einer .bin-Datei und 
und kann von der \textit{Inferecne Egine} 
zur Inferenz gelesen werden.


\subsection*{InferecneEgine}

Um die Inferenz eines Modells im IR-Format 
auf dem NCS2 ausführen zu können, werden 
in der \textit{Inference Engine}, die in Abbildung 
\ref{fig:inger_engine_workflow} schematisch
dargestellten Schritte, durchgeführt.
Daneben ist jeweils die entsprechende 
Codezeile in Python dargestellt.

Zunächst wird das Zielgerät, auf dem 
die Inferenz ausgeführt werden soll,
spezifiziert (\textit{HW Plugin laden}).

Anschließend wird das Modell anhand der 
IR Dateien definiert (\textit{Model IR einlesen})
woraus sich die \textit{In}- und \textit{Outputblobs}
 generieren lassen, (\textit{In-und Outputblob}), 
welche die Diemensionen der Ein- und Ausgabe 
Schicht des Modells darstellen.

Das zu inferierende Bild,
welches als Matrix aus Pixelwerten 
vorliegt, muss dann in das \textit{Input Blob}
Format gebracht werden (\textit{process Input}).

Nachdem das Bild inferiert wurde (\textit{Inferenz}),
kann es, zusammen mit den Inferenzergebnissen, 
weiterverarbeitet werden (\textit{process
output}).

Handelt es sich bei den Inputs 
um einen fortlaufenden Video- oder 
Kamera Stream, werden die Schritte 
\textit{preprocess}, \textit{Inferenz} und 
\textit{process Output} in einer Schleife wiederholt.


\vspace{1cm}
\begin{minipage}{0.30\textwidth}
    \centering
    \input{Bilder/inferece_engine_diagram}
    \captionof{figure}{Ablauf der InferenceEngine}
    \label{fig:inger_engine_workflow}
\end{minipage}
\begin{minipage}{0.70\textwidth}

%\begin{python}
\begin{lstlisting}[language=Python]

    plugin = IEPlugin(device='MYRIAD')

        
    net = IENetwork(model_xml, model_bin)
        
    
    input_blob  = net.inputs
    output_blob = net.outputs
        

    exec_net = plugin.load_network(net, n_req)
        
    while True:

        image = preprocess(capture) # hwc -> nchw
        
        
        res = exec_net.infer({input_blob : image})
        

        res = res[output_blob]
        
        
\end{lstlisting}
%\end{python}
\vspace{1.5cm}
\end{minipage}
\vspace{1cm}

% transform:
% # capture dims zu input blob dims transformieren
% # img_h, img_w, img_c -> blob_n, blob_c, blob_h, blob_w

Die Form des Inferenzergebnisses hängt von der 
Art des verwendeten Deep Learning Modells ab, welche 
z.B. \textit{Image Classification}, \textit{Object detection},
oder \textit{Instance
Segmentation} sein können.

Für Object detection Modelle enthällt das Ergebnis
 Datenstrukturen, welche den Index, 
 zugehörige Wahrscheinlichkeit, sowie Bounding 
 Box Koordinaten der geschätzen Objekte im Bild enthalten.

Indem für alle Schätzungen, die in einem Bild gemacht wurden, 
ein Threshhold für die Wahrscheinlichkeit festlegt wird (zb. 70\%),
könenn die sinnvollen Ergebnisse herausgefiltert werden.

Zur veranschaulichung können die Koordinaten dann 
als Bounding Box in das inferierte Bild gezeichnet werden.
