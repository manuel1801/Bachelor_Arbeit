\chapter{Zusammenfassung}\label{kap:zusammenfassungausblick}


Ziel der Arbeit war es, ein autonomes
Kamerasystem zur Wildtierkennung
mithilfe Neuronaler Netze zu entwickeln.
Dafür wurden vortrainierte, \Gls{cnn}-basierte, 
Deep-Learning-Modelle zur Objekterkennung
auf einen geeigneten Datensatz trainiert.
Dadurch sollte es möglich sein, nicht 
nur die Anwesenheit eines Tieres 
zu erkennen, sondern auch eine 
Klassifizierung der Tierart 
vorzunehmen. Auf diese Weise kann das System 
gezielter für eine bestimmte
Anwendung eingesetzt werden.
Zur Realisierung wurde neben einem \textit{Raspberry
Pi} sowie einer nachtsichtgeeigneten 
Kamera der \textit{Neural Compute Stick 2}
von \textit{Intel} verwendet, um die 
Verarbeitung der Daten auf dem 
Gerät ausführen zu können.
\vspace{0.5cm}


Für das Training wurde ein Datensatz, 
bestehend aus 9 Wildtierklassen verwendet,
welcher aus \textit{Open Images} heruntergeladen werden konnte.
Anschließend wurden die Daten 
für das Training aufbereitet und
zur Evaluierung in verschiedene 
Sets aufgeteilt.
Das Training wurde dann mithilfe des 
\Glspl{framework} \textit{TensorFlow} durchgeführt,
wobei die Modelle Single Shot Detector (SSD) und
Faster R-CNN mit verschiedenen
Basis-\Glspl{cnn} und Parametereinstellungen
verwendet wurden.
Durch anschließende Evaluierung konnte 
festgestellt werden, welches Modell sich,
bezogen auf Genauigkeit und Geschwindigkeit,
am besten für die Anwendung eignet.
Der letzte Schritt war die Inferenz zusammen 
mit dem Anwendungscode für den \textit{Raspberry Pi} 
zu implementieren, wofür mit \textit{OpenVino} gearbeitet 
wurde.
\vspace{0.5cm}

Die Evaluierung der trainierten Modelle zeigte,
dass eine erhöhte Genauigkeit mit einer 
langsameren Inferenzzeit einhergeht.
Durch umfangreiche Testläufe mit 
variierenden Parametern konnten die 
optimalen Konfigurationen für die Anwendung 
erforscht und somit die Ergebnisse verbessert 
werden.

Daraus resultierend wurde für die Anwendung das 
Faster R-CNN Modell mit InceptionV2 
als Basis-\Gls{cnn} gewählt. Dieses erreichte 
durch ein Training von 500k Iterationen 
auf den \textit{Open Images} Datensatz 
einen mAP-Wert von 0.7 und einen Loss-Wert von 0.74.
Der Datensatz wurde durch geometrische und 
pixelwertbezogene Veränderungen der Bilder 
augmentiert, wodurch für jede Klasse 3000 
Bilder für das Training vorhanden waren.

Durch einen asynchronen Inferenzablauf 
mit drei Inferenz-Requests konnte die 
Inferenzzeit für das Faster R-CNN 
von 0,63 \Gls{fps} auf 0,75 \Gls{fps} 
erhöht werden. 
Indem ein Bewegungsmelder sowie ein 
Zwischenspeichern der Frames im 
Anwendungsablauf implementiert wurde, ließ sich 
die Performance weiter verbessern.
